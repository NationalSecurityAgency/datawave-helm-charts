####################################
# Core Helm Template Configuration #
####################################

apiVersion: v1
kind: ConfigMap

######################################
# Basic Metadata for this Deployment #
######################################

metadata:
  name: "{{ .Chart.Name }}-ingest-env-configmap"

##########################
# Configuration Map Data #
##########################

data: 

  #################################################
  # Kubernetes-Specific Application Configuration #
  #################################################

  ingest-env.sh: |
    #!/bin/bash
    
    #Directory in which to place the lock files
    export LOCK_FILE_DIR={{ .Values.ingest.config.lockFileDir}}
    
    if [[ ! -d ${LOCK_FILE_DIR} || ! -w ${LOCK_FILE_DIR} ]]; then
      "Lock file directory ${LOCK_FILE_DIR} does not exist or is not writable. Exiting..."
      exit 1
    fi
    
    . ../system/header.sh
    
    . ../ingest/tables-env.sh
    
    # regex matching changed since bash 3.1....ensure we are forward compatible
    shopt -s compat31 > /dev/null 2>&1
    
    # load the external password specifications.  The following needs to be defined in this script: PASSWORD, TRUSTSTORE_PASSWORD, KEYSTORE_PASSWORD
    function checkForVar (){
       found=`cat $1 | egrep " $2 *="`
       if [[ "$found" == "" ]]; then
          echo "$2,"
       fi
    }
    
    PASSWORD_INGEST_ENV="/opt/datawave-ingest/ingest-passwd.sh"
    if [[ "$PASSWORD_INGEST_ENV" != "" ]]; then
       if [[ -e /opt/datawave-ingest/ingest-passwd.sh ]]; then
          missing=\
    "$(checkForVar $PASSWORD_INGEST_ENV "PASSWORD")\
    $(checkForVar $PASSWORD_INGEST_ENV "TRUSTSTORE_PASSWORD")\
    $(checkForVar $PASSWORD_INGEST_ENV "KEYSTORE_PASSWORD")"
          if [[ "$missing" != "" ]]; then
             echo "FATAL: /opt/datawave-ingest/ingest-passwd.sh is missing the following definitions: $missing"
             exit 10
          fi
          . "$PASSWORD_INGEST_ENV"
       else
          echo "FATAL: /opt/datawave-ingest/ingest-passwd.sh was not found  Please create that script on this system."
          exit 10
       fi
    else
       echo "FATAL: PASSWORD_INGEST_ENV was not defined.  Please define this in your deployment properties and create that script on this system."
       echo "   e.g. /opt/datawave-ingest/ingest-passwd.sh:"
       echo "        export PASSWORD=\"accumulo_passwd\""
       echo "        export TRUSTSTORE_PASSWORD=\"trust_passwd\""
       echo "        export KEYSTORE_PASSWORD=\"cert_passwd\""
       exit 10
    fi
    
    # if a deployment specific environment has been specified, then load it
    ADDITIONAL_INGEST_ENV="${ADDITIONAL_INGEST_ENV}"
    if [[ "$ADDITIONAL_INGEST_ENV" != "" ]]; then
       . "$ADDITIONAL_INGEST_ENV"
    fi
    
    ADDITIONAL_INGEST_LIBS="${ADDITIONAL_INGEST_LIBS}"
    
    # NOTE:
    # Make sure to use the following pattern when setting variables and you want to assign them default values.
    #
    # (1) VARIABLE_X=${VARIABLE_X}
    # (2) VARIABLE_X=${VARIABLE_X:-<default value>}
    #
    # - The first assignment is used by maven during compile to set variables to the value defined in the
    #   default.properties (or dev.properties, etc.) files.
    # - The second assignment sets a variable to a "default" value in the event that it was not set in the
    #   first (previous) assignment.
    #
    
    # Provides a method to run map-file-bulk-loader as a different user
    MAP_FILE_LOADER_COMMAND_PREFIX="${MAP_FILE_LOADER_COMMAND_PREFIX}"
    MAP_FILE_LOADER_EXTRA_ARGS="-ingestMetricsDisabled"
    MAP_FILE_LOADER_SEPARATE_START="${MAP_FILE_LOADER_SEPARATE_START}"
    MAP_FILE_LOADER_SEPARATE_START="${MAP_FILE_LOADER_SEPARATE_START:-false}"
    RCPT_TO="hadoop@localhost"
    SEND_JOB_EMAIL_DISABLED="${SEND_JOB_EMAIL_DISABLED}"
    
    HADOOP_HOME="{{ .Values.ingest.config.hadoopHome }}"
    MAPRED_HOME="{{ .Values.ingest.config.mapredHome }}"
    MAPRED_HOME="${MAPRED_HOME:-$HADOOP_HOME}"
    HADOOP_CONF_DIR="/usr/local/hadoop/conf"
    USERNAME="{{ .Values.ingest.config.accumuloUser }}"
    
    WAREHOUSE_ACCUMULO_HOME="{{ .Values.ingest.config.accumuloDir }}"
    WAREHOUSE_ACCUMULO_LIB=""
    WAREHOUSE_ACCUMULO_BIN=""
    WAREHOUSE_ACCUMULO_LIB="${WAREHOUSE_ACCUMULO_LIB:-$WAREHOUSE_ACCUMULO_HOME/lib}"
    WAREHOUSE_ACCUMULO_BIN="${WAREHOUSE_ACCUMULO_BIN:-$WAREHOUSE_ACCUMULO_HOME/bin}"
    WAREHOUSE_HDFS_NAME_NODE="{{ .Values.ingest.config.hdfsNamenode }}"
    WAREHOUSE_NAME_BASE_DIR="${WAREHOUSE_NAME_BASE_DIR}"
    WAREHOUSE_JOBTRACKER_NODE="{{ .Values.ingest.config.jobTracker }}"
    WAREHOUSE_ZOOKEEPERS="{{ .Values.ingest.config.zookeeper }}"
    WAREHOUSE_INSTANCE_NAME="{{ .Values.ingest.config.instanceName }}"
    # setting these two times may seem unnecessary, but the first one is required if
    # the property is set in the assembly properties (see datawave_deploy).  The second
    # one is needed if it is not set explicitly but HADOOP_HOME is.
    WAREHOUSE_HADOOP_HOME="${WAREHOUSE_HADOOP_HOME}"
    WAREHOUSE_HADOOP_HOME="${WAREHOUSE_HADOOP_HOME:-$HADOOP_HOME}"
    WAREHOUSE_MAPRED_HOME="${WAREHOUSE_MAPRED_HOME}"
    WAREHOUSE_MAPRED_HOME="${WAREHOUSE_MAPRED_HOME:-$MAPRED_HOME}"
    WAREHOUSE_HADOOP_CONF="{{ .Values.ingest.config.hadoopConfDif }}"
    WAREHOUSE_HADOOP_CONF="${WAREHOUSE_HADOOP_CONF:-$WAREHOUSE_HADOOP_HOME/conf}"
    
    INGEST_ACCUMULO_HOME="{{ .Values.ingest.config.accumuloDir }}"
    INGEST_HDFS_NAME_NODE="{{ .Values.ingest.config.hdfsNamenode }}"
    INGEST_JOBTRACKER_NODE="{{ .Values.ingest.config.jobTracker }}"
    INGEST_ZOOKEEPERS="{{ .Values.ingest.config.zookeeper }}"
    INGEST_INSTANCE_NAME="{{ .Values.ingest.config.instanceName }}"
    # setting these two times may seem unnecessary, but the first one is required if
    # the property is set in the assembly properties (see datawave_deploy).  The second
    # one is needed if it is not set explicitly but HADOOP_HOME is.
    INGEST_HADOOP_HOME="${INGEST_HADOOP_HOME}"
    INGEST_HADOOP_HOME="${INGEST_HADOOP_HOME:-$HADOOP_HOME}"
    INGEST_MAPRED_HOME="${INGEST_MAPRED_HOME}"
    INGEST_MAPRED_HOME="${INGEST_MAPRED_HOME:-$MAPRED_HOME}"
    INGEST_HADOOP_CONF="{{ .Values.ingest.config.hadoopConfDif }}"
    INGEST_HADOOP_CONF="${INGEST_HADOOP_CONF:-$INGEST_HADOOP_HOME/conf}"
    
    # STAGING_HOSTS is a comma delimited list of hosts
    STAGING_HOSTS="`nodeattr -c staging`"
    INGEST_HOST="ingestmaster"
    ROLLUP_HOST="ingestmaster"
    
    # hadoop and child opts for ingest
    MAPRED_INGEST_OPTS="-useInlineCombiner"
    HADOOP_INGEST_OPTS=""
    CHILD_INGEST_OPTS=""
    
    LIVE_CHILD_MAX_MEMORY_MB="${LIVE_CHILD_MAX_MEMORY_MB}"
    BULK_CHILD_MAX_MEMORY_MB="${BULK_CHILD_MAX_MEMORY_MB}"
    MISSION_MGMT_CHILD_MAP_MAX_MEMORY_MB="${MISSION_MGMT_CHILD_MAP_MAX_MEMORY_MB}"
    
    # The next two comma delimited lists work in concert with each other and must align
    CONFIG_DATA_TYPES="${CONFIG_DATA_TYPES}"
    CONFIG_FILES="${CONFIG_FILES}"
    if [[ "$CONFIG_FILES" == "" ]]; then
        # attempt to create the CONFIG_DATA_TYPES and CONFIG_FILES by scanning the config directory
        for config_file in ../../config/*.xml; do
            CONFIG_DATA_TYPE=`grep -A 1 -B 1 '>data.name<' $config_file | grep '<value>' | sed 's/.*<value>//' | sed 's/<\/value>.*//' | sed 's/\.//'`
            if [[ "$CONFIG_DATA_TYPE" != "" ]]; then
                CONFIG_DATA_TYPES=$CONFIG_DATA_TYPE,$CONFIG_DATA_TYPES
                CONFIG_FILES=${config_file##*/},$CONFIG_FILES
            fi
        done
    fi
    
    BULK_MAP_OUTPUT_COMPRESS=${BULK_MAP_OUTPUT_COMPRESS}
    BULK_MAP_OUTPUT_COMPRESS=${BULK_MAP_OUTPUT_COMPRESS:-true}
    BULK_MAP_OUTPUT_COMPRESSION_CODEC=${BULK_MAP_OUTPUT_COMPRESSION_CODEC}
    BULK_MAP_OUTPUT_COMPRESSION_CODEC=${BULK_MAP_OUTPUT_COMPRESSION_CODEC:-org.apache.hadoop.io.compress.DefaultCodec}
    BULK_MAP_OUTPUT_COMPRESSION_TYPE=${BULK_MAP_OUTPUT_COMPRESSION_TYPE}
    BULK_MAP_OUTPUT_COMPRESSION_TYPE=${BULK_MAP_OUTPUT_COMPRESSION_TYPE:-BLOCK}
    
    LIVE_MAP_OUTPUT_COMPRESS=${LIVE_MAP_OUTPUT_COMPRESS}
    LIVE_MAP_OUTPUT_COMPRESS=${LIVE_MAP_OUTPUT_COMPRESS:-true}
    LIVE_MAP_OUTPUT_COMPRESSION_CODEC=${LIVE_MAP_OUTPUT_COMPRESSION_CODEC}
    LIVE_MAP_OUTPUT_COMPRESSION_CODEC=${LIVE_MAP_OUTPUT_COMPRESSION_CODEC:-org.apache.hadoop.io.compress.DefaultCodec}
    LIVE_MAP_OUTPUT_COMPRESSION_TYPE=${LIVE_MAP_OUTPUT_COMPRESSION_TYPE}
    LIVE_MAP_OUTPUT_COMPRESSION_TYPE=${LIVE_MAP_OUTPUT_COMPRESSION_TYPE:-RECORD}
    
    BULK_INGEST_DATA_TYPES="{{ .Values.ingest.config.bulkDataTypes }}"
    LIVE_INGEST_DATA_TYPES="{{ .Values.ingest.config.liveDataTypes  }}"
    MISSION_MGMT_DATA_TYPES="${MISSION_MGMT_DATA_TYPES}"
    
    BULK_INGEST_REDUCERS="10"
    LIVE_INGEST_REDUCERS="10"
    
    declare -i INGEST_BULK_MAPPERS=4
    declare -i INGEST_MAX_BULK_BLOCKS_PER_JOB=4
    declare -i INGEST_LIVE_MAPPERS=4
    declare -i INGEST_MAX_LIVE_BLOCKS_PER_JOB=4
    
    MAP_LOADER_HDFS_NAME_NODES="${MAP_LOADER_HDFS_NAME_NODES}"
    MAP_LOADER_HDFS_NAME_NODES="${MAP_LOADER_HDFS_NAME_NODES:-$WAREHOUSE_HDFS_NAME_NODE}"
    NUM_MAP_LOADERS="1"
    NUM_MAP_LOADERS="${NUM_MAP_LOADERS:-1}"
    
    ZOOKEEPER_HOME="/usr/lib/zookeeper"
    
    JAVA_HOME="/usr/lib/jvm/java/"
    PYTHON="/usr/bin/python"
    
    HDFS_BASE_DIR="/data"
    
    BASE_WORK_DIR="${BASE_WORK_DIR}"
    BASE_WORK_DIR="${BASE_WORK_DIR:-/datawave/ingest/work}"
    
    HDFS_MONITOR_ARGS="${HDFS_MONITOR_ARGS}"
    
    MONITOR_SERVER_HOST="monitor"
    MONITOR_ENABLED="${MONITOR_ENABLED}"
    MONITOR_ENABLED="${MONITOR_ENABLED:-true}"
    
    LOG_DIR="/srv/logs/ingest"
    PDSH_LOG_DIR="/srv/logs/ingest/pdsh_logs"
    FLAG_DIR="/srv/data/datawave/flags"
    BIN_DIR_FOR_FLAGS="/opt/datawave-ingest/current/bin"
    FLAG_MAKER_CONFIG="/opt/datawave-ingest/current/config/flag-maker-live.xml,/opt/datawave-ingest/current/config/flag-maker-bulk.xml"
    FLAG_EXTRA_ARGS=""
    
    declare -i NUM_SHARDS=${NUM_SHARDS}
    declare -i NUM_DATE_INDEX_SHARDS=${NUM_DATE_INDEX_SHARDS}
    
    MAP_LOADER_MAJC_THRESHOLD="${MAP_LOADER_MAJC_THRESHOLD}"
    MAP_LOADER_MAJC_THRESHOLD="${MAP_LOADER_MAJC_THRESHOLD:-32000}"
    
    # using ../ingest instead of ./ allows scripts in other bin directories to use this
    findVersion (){
      ls -1 $1/$2-*.jar | grep -v sources | grep -v javadoc | sort | tail -1 | sed 's/.*\///' | sed "s/$2-//" | sed 's/.jar//'
    }
    findHadoopVersion (){
      $1/bin/hadoop version | head -1 | awk '{print $2}'
    }
    METRICS_VERSION=$(findVersion ../../lib datawave-metrics-core)
    INGEST_VERSION=$(findVersion ../../lib datawave-ingest-csv)
    HADOOP_VERSION=$(findHadoopVersion $INGEST_HADOOP_HOME)
    
    
    # Turn some of the comma delimited lists into arrays
    OLD_IFS="$IFS"
    IFS=","
    FLAG_MAKER_CONFIG=( $FLAG_MAKER_CONFIG )
    CONFIG_DATA_TYPES=( $CONFIG_DATA_TYPES )
    CONFIG_FILES=( $CONFIG_FILES )
    MAP_LOADER_HDFS_NAME_NODES=( $MAP_LOADER_HDFS_NAME_NODES )
    NUM_MAP_LOADERS=( $NUM_MAP_LOADERS )
    IFS="$OLD_IFS"
    
    # Export the variables as needed (required by some python scripts and some java code)
    export LOG_DIR
    export FLAG_DIR
    export BIN_DIR_FOR_FLAGS
    export INGEST_HDFS_NAME_NODE HDFS_BASE_DIR BASE_WORK_DIR
    export INGEST_BULK_MAPPERS INGEST_MAX_BULK_BLOCKS_PER_JOB
    export INGEST_LIVE_MAPPERS INGEST_MAX_LIVE_BLOCKS_PER_JOB
    export BULK_INGEST_REDUCERS LIVE_INGEST_REDUCERS MISSION_MGMT_INGEST_REDUCERS
    export BULK_INGEST_DATA_TYPES LIVE_INGEST_DATA_TYPES MISSION_MGMT_DATA_TYPES
    export MONITOR_SERVER_HOST MONITOR_ENABLED
    export PYTHON INGEST_HADOOP_HOME WAREHOUSE_HADOOP_HOME JAVA_HOME
    export NUM_SHARDS NUM_DATE_INDEX_SHARDS
    export INDEX_STATS_MAX_MAPPERS
    
    BULK_CHILD_MAP_MAX_MEMORY_MB="2048"
    BULK_CHILD_MAP_MAX_MEMORY_MB="${BULK_CHILD_MAP_MAX_MEMORY_MB:-$BULK_CHILD_MAX_MEMORY_MB}"
    LIVE_CHILD_MAP_MAX_MEMORY_MB="1024"
    LIVE_CHILD_MAP_MAX_MEMORY_MB="${LIVE_CHILD_MAP_MAX_MEMORY_MB:-$LIVE_CHILD_MAX_MEMORY_MB}"
    
    BULK_CHILD_REDUCE_MAX_MEMORY_MB="2048"
    BULK_CHILD_REDUCE_MAX_MEMORY_MB="${BULK_CHILD_REDUCE_MAX_MEMORY_MB:-$BULK_CHILD_MAX_MEMORY_MB}"
    LIVE_CHILD_REDUCE_MAX_MEMORY_MB="1024"
    LIVE_CHILD_REDUCE_MAX_MEMORY_MB="${LIVE_CHILD_REDUCE_MAX_MEMORY_MB:-$LIVE_CHILD_MAX_MEMORY_MB}"
    
    DEFAULT_IO_SORT_MB="${DEFAULT_IO_SORT_MB}"
    DEFAULT_IO_SORT_MB="${DEFAULT_IO_SORT_MB:-"768"}"
    BULK_CHILD_IO_SORT_MB="${BULK_CHILD_IO_SORT_MB}"
    BULK_CHILD_IO_SORT_MB="${BULK_CHILD_IO_SORT_MB:-$DEFAULT_IO_SORT_MB}"
    LIVE_CHILD_IO_SORT_MB="${LIVE_CHILD_IO_SORT_MB}"
    LIVE_CHILD_IO_SORT_MB="${LIVE_CHILD_IO_SORT_MB:-$DEFAULT_IO_SORT_MB}"
    
    COMPOSITE_INGEST_DATA_TYPES=""
    DEPRECATED_INGEST_DATA_TYPES=""
    
    BULK_INGEST_GROUPING="${BULK_INGEST_GROUPING}"
    BULK_INGEST_GROUPING="${BULK_INGEST_GROUPING:-none}"
    LIVE_INGEST_GROUPING="${LIVE_INGEST_GROUPING}"
    LIVE_INGEST_GROUPING="${LIVE_INGEST_GROUPING:-none}"
    
    INGEST_BULK_JOBS="1"
    INGEST_LIVE_JOBS="1"
    
    BULK_INGEST_TIMEOUT_SECS="${BULK_INGEST_TIMEOUT_SECS}"
    BULK_INGEST_TIMEOUT_SECS="${BULK_INGEST_TIMEOUT_SECS:-300}"
    LIVE_INGEST_TIMEOUT_SECS="${LIVE_INGEST_TIMEOUT_SECS}"
    LIVE_INGEST_TIMEOUT_SECS="${LIVE_INGEST_TIMEOUT_SECS:-10}"
    
    declare -i INDEX_STATS_MAX_MAPPERS=1
    
    # Export the variables as needed (required by some python scripts and some java code)
    export INGEST_BULK_JOBS
    export INGEST_LIVE_JOBS
    export BULK_INGEST_GROUPING LIVE_INGEST_GROUPING
    export BULK_INGEST_TIMEOUT_SECS LIVE_INGEST_TIMEOUT_SECS
    
    # CERT required for various download scripts (PEM format)
    export SERVER_CERT="${SERVER_CERT}"
    export KEYSTORE="/data/certs/keystore.p12"
    export KEYSTORE_TYPE="PKCS12"
    export TRUSTSTORE="/data/certs/truststore.jks"
    export TRUSTSTORE_TYPE="JKS"
    
    
    LOAD_JOBCACHE_CPU_MULTIPLIER="${LOAD_JOBCACHE_CPU_MULTIPLIER}"
    declare -i LOAD_JOBCACHE_CPU_MULTIPLIER=${LOAD_JOBCACHE_CPU_MULTIPLIER:-2}
    
    # some functions used by script to parse flag file names
    
    isNumber() {
      re='^[0-9]+$'
      if [[ $1 =~ $re ]]; then
        echo "true"
      else
        echo "false"
      fi
    }
    
    flagPipeline() {
      BASENAME=${1%.*}
      PIPELINE=${BASENAME##*.}
      if [[ $(isNumber $PIPELINE) == "true" ]]; then
        echo $PIPELINE
      else
        echo 0
      fi
    }
    
    flagBasename() {
      f=$1
      BASENAME=${f%%.flag*}
      echo $BASENAME
    }
    