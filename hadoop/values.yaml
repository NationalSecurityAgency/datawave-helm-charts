global:
  pullSecrets:
  - dockerconfigjson-ghcr

<<<<<<< HEAD
nameOverride: hadoop

# The base hadoop image to use for all components.
# See this repo for image build details: https://github.com/Comcast/kube-yarn/tree/manager/image
image:
  repository: ghcr.io/nationalsecurityagency/datawave-stack-hadoop
  tag: 3.3.6
  pullPolicy: IfNotPresent

# The version of the hadoop libraries being used in the image.
hadoopVersion: 3.3.6-2

# Select antiAffinity as either hard or soft, default is soft
antiAffinity: "soft"
config:
  postInstallCommands:
    - hdfs dfs -mkdir -p hdfs://hdfs-nn:9000/accumulo
    - hdfs dfs -chown accumulo hdfs://hdfs-nn:9000/accumulo
    - hdfs dfs -chmod 700 hdfs://hdfs-nn:9000/accumulo
    - hdfs dfs -chmod 777 hdfs://hdfs-nn:9000/
    - hdfs dfs -chmod -R 777 hdfs://hdfs-nn:9000/data
    - hdfs dfs -chmod -R 777 hdfs://hdfs-nn:9000/datawave
    - hdfs dfs -mkdir -p hdfs://hdfs-nn:9000/tmp/hadoop-yarn/staging/history
    - hdfs dfs -chmod -R 777 hdfs://hdfs-nn:9000/tmp/
  ec:
    enabled: true
    postInstallCommands: 
      - export HADOOP_CONF_DIR=/tmp/hadoop-config
      - hdfs ec -addPolicies -policyFile /tmp/hadoop-config/erasure-coding-policies.xml
      - echo "done adding start enabling"
      - hdfs ec -enablePolicy -policy RS-6-3-64k
      - hdfs ec -enablePolicy -policy RS-6-3-128k
      - hdfs ec -enablePolicy -policy RS-6-2-64k
      - hdfs ec -enablePolicy -policy RS-6-2-128k
      - hdfs ec -enablePolicy -policy RS-4-2-64k
      - hdfs ec -enablePolicy -policy RS-4-2-128k
      - hdfs ec -enablePolicy -policy RS-4-3-64k
      - hdfs ec -enablePolicy -policy RS-4-3-128k
  coreSiteProperties:
    - name: fs.defaultFS
      value: hdfs://hdfs-nn:9000/
      description: NameNode URI
    - name: dfs.permissions.supergroup
      value: hadoop
      description: Hadoop Super User Group

  hdfsSiteProperties:
    - name: dfs.webhdfs.enabled
      value: false
      description: whether to start webhdfs
    - name: dfs.datanode.use.datanode.hostname
      value: false
      description: Use hostname for datanode
    - name: dfs.client.use.datanode.hostname
      value: false
      description: Use hostname for datanode from client
    - name: dfs.replication
      value: "3"
      description: Default Replication
    - name: dfs.datanode.data.dir
      value: file:///usr/local/hadoop/hdfs/datanode
      description: Datanode Data Directory
    - name: dfs.namenode.name.dir
      value: file:///usr/local/hadoop/data
      description: NameNode directory for namespace and transaction logs storage.
    - name: dfs.namenode.datanode.registration.ip-hostname-check
      value: false
      description: Disable hostname ip check
    - name: dfs.namenode.rpc-bind-host
      value: "0.0.0.0"
      description: Bind Namenode to all hosts
    - name: dfs.namenode.servicerpc-bind-host
      value: "0.0.0.0"
      description: Bind Namenode to all hosts
    - name: dfs.datanode.fileio.profiling.sampling.percentage
      value: "20"
      description: "Percentage of disk io to profile"

  mapredSiteProperties:
    - name: mapreduce.framework.name
      value: yarn
      description: What Framework to use
    - name: mapreduce.map.memory.mb
      value: "4096"
      description: "Default max task size"
    - name: mapreduce.jobhistory.address
      value: yarn-rm:10020
      description: Resource Manager Address
    - name: mapreduce.jobhistory.webapp.address
      value: "0.0.0.0:19888"
      description: Bind RM to all hosts
    - name: mapreduce.task.profile	
      value: true
      description: "Enable Profiling"
    - name: mapreduce.task.profile.maps	
      value: 2
      description: "This takes 0-2. Not sure what the numbers are, but this is what I expect"

  yarnSiteProperties:
    - name: yarn.resourcemanager.hostname
      value: yarn-rm
      description: Resource Manager dns name
    - name: yarn.resourcemanager.bind-host
      value: "0.0.0.0"
      description: Bind RM to all interfaces
    - name: yarn.nodemanager.bind-host
      value: "0.0.0.0"
      description: Bind NM to all interfaces
    - name: yarn.timeline-service.bind-host
      value: "0.0.0.0"
      description: Bind TS to all interfaces
    - name: yarn.nodemanager.vmem-check-enabled
      value: false
      description: Disable VMEM Check
    - name: yarn.nodemanager.aux-services
      value: mapreduce_shuffle
      description: Node Manager Auxiliary Services
    - name: yarn.nodemanager.aux-services.mapreduce_shuffle.class
      value: org.apache.hadoop.mapred.ShuffleHandler
      description: Class to use for mapreduce_shuffle
    - name: yarn.nodemanager.resource.memory-mb
      value: "4096"
      description: Memory size for each node manager
    - name: yarn.nodemanager.local-dirs
      value: /opt/hdfs/hadoop-yarn/cache/${user.name}/nm-local-dir
      description: Nodemanager local directory
    - name: yarn.nodemanager.log-dirs
      value: /opt/hdfs/hadoop-yarn/containers
      description: Nodemanager local logs directory
    - name: yarn.nodemanager.remote-app-log-dir
      value: /opt/hdfs/hadoop-yarn/contaiappsners
      description: Nodemanager log aggregation directory
    - name: yarn.resourcemanager.scheduler.class
      value: org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler
      description: Scheduler Class to use
    - name: yarn.application.classpath
      value: /opt/hadoop/conf,/usr/lib/hadoop/*,/usr/lib/hadoop/lib/*,/usr/lib/hadoop-hdfs/*,/usr/lib/hadoop-hdfs/lib/*,/usr/lib/hadoop-yarn/*,/usr/lib/hadoop-yarn/lib/*,/usr/lib/hadoop-yarn/timelineservice/*,/usr/lib/hadoop-mapreduce/*,
      description: Classpath for Yarn jobs

hdfs:
  nameNode:
    ingress:
      enabled: true
      annotations: []
      host: "namenode.datawave.org"
    tolerations: []

    resources:
      requests:
        memory: "256Mi"
        cpu: "10m"
      limits:
        memory: "2048Mi"
        cpu: "1000m"

  dataNode:
    tolerations: []
    replicas: 2

    resources:
      requests:
        memory: "256Mi"
        cpu: "10m"
      limits:
        memory: "2048Mi"
        cpu: "1000m"

  webhdfs:
    enabled: false

yarn:
  historyServer:
    ingress:
      enabled: true
      annotations: []
      host: "historyserver.datawave.org"
  resourceManager:
    ingress:
      enabled: true
      annotations: []
      host: "resourcemanager.datawave.org"
    tolerations: []

    resources:
      requests:
        memory: "256Mi"
        cpu: "10m"
      limits:
        memory: "2048Mi"
        cpu: "2000m"

  nodeManager:
    tolerations: []

    # The number of YARN NodeManager instances.
    replicas: 2

    # Create statefulsets in parallel (K8S 1.7+)
    parallelCreate: false

    # CPU and memory resources allocated to each node manager pod.
    # This should be tuned to fit your workload.
    resources:
      requests:
        memory: "2048Mi"
        cpu: "1000m"
      limits:
        memory: "4096Mi"
        cpu: "1000m"
cmds:
  resources: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

persistence:
  nameNode:
    enabled: false
    definitions:
    - metadata:
        name: dfs
      spec:
        accessModes: ReadWriteOnce
        storageClassName: managed-csi-driver
        resources:
          requests:
            storage: 50Gi
        selector:

  dataNode:
    enabled: false
    definitions:
    - metadata:
        name: dfs
      spec:
        accessModes: ReadWriteOnce
        storageClassName: managed-csi-driver
        resources:
          requests:
            storage: 200Gi
        selector: