{{- define "datawave.datawaveInitContainer" -}}
#!/bin/bash -e

DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
cd $DIR

export ROOT_DATAWAVE_HADOOP_HDFS_HOME=hdfs://hdfs-nn:9000/datawave/
export DW_ACCUMULO_VFS_DATAWAVE_DIR=${ROOT_DATAWAVE_HADOOP_HDFS_HOME}accumulo-vfs-classpath
export HADOOP_HOME=/usr/local/hadoop
export HADOOP_CONF_DIR=/usr/local/hadoop/conf
export HADOOP_YARN_HOME=$HADOOP_HOME
export DW_DATAWAVE_INGEST_HOME=/opt/datawave-ingest/current/
echo "Creating temp dir in hdfs"
hdfs dfs -mkdir -p hdfs://hdfs-nn:9000/tmp/hadoop-yarn
hdfs dfs -chmod -R -777 hdfs://hdfs-nn:9000/tmp/

# Create the data ingest folder for the datatypes
{{- range $key := .Values.config.types }}
datatypes="$datatypes {{ $key.flagMakerConfig.liveFolder }}"
{{- end }}
echo $datatypes
for data_type in $datatypes;
do
  echo "Creating $data_type dir"
  hdfs dfs -mkdir -p hdfs://hdfs-nn:9000/data/$data_type
  hdfs dfs -chmod -R 777 hdfs://hdfs-nn:9000/data/$data_type
done

echo "Creating classpath dir"
hdfs dfs -mkdir -p "${DW_ACCUMULO_VFS_DATAWAVE_DIR}"

echo "Updating permissions on /datawave in hdfs"
hdfs dfs -chown datawave "${ROOT_DATAWAVE_HADOOP_HDFS_HOME}"

echo "Updating permissions on /data in hdfs"
hdfs dfs -chown -R datawave hdfs://hdfs-nn:9000/data
hdfs dfs -chmod -R 777 hdfs://hdfs-nn:9000/data/myjson
echo "Copying DataWave jars into HDFS dir: ${DW_ACCUMULO_VFS_DATAWAVE_DIR}"
if [ -d ${DW_DATAWAVE_INGEST_HOME}/accumulo-warehouse/lib ]; then
  hdfs dfs -put -f ${DW_DATAWAVE_INGEST_HOME}/accumulo-warehouse/lib/*.jar ${DW_ACCUMULO_VFS_DATAWAVE_DIR}
fi
if [ -d ${DW_DATAWAVE_INGEST_HOME}/accumulo-warehouse/lib/ext ]; then
  hdfs dfs -put -f ${DW_DATAWAVE_INGEST_HOME}/accumulo-warehouse/lib/ext/*.jar ${DW_ACCUMULO_VFS_DATAWAVE_DIR}
fi

{{- end -}}

apiVersion: v1
kind: Secret
metadata:
  name: {{ include "datawave.fullname" . }}-initcontainer-cmds
  labels:
    {{- include "datawave.labels" . | nindent 4 }}
type: Opaque
data:
  run.sh: {{ include "datawave.datawaveInitContainer" . | b64enc }}
