global:
  pullSecrets:
    - dockerconfigjson-ghcr
  hadoopConfigMap: hadoop-config
  accumuloConfigMap: accumulo-config
nameOverride: ""
fullnameOverride: ""
labels: {}
image:
  repository: ghcr.io/nationalsecurityagency/datawave/ingest-kubernetes
  tag: 7.5.0-SNAPSHOT
  pullPolicy: IfNotPresent
deployment:
  replicaCount: 1
  resources: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}
libs: "/lib/hadoop-mapreduce/hadoop-mapreduce-client-core.jar:/lib/hadoop-hdfs/hadoop-hdfs-client.jar:/lib/hadoop-mapreduce/hadoop-mapreduce-client-common.jar:/lib/hadoop-mapreduce/hadoop-mapreduce-client-jobclient.jar:/lib/hadoop-mapreduce/hadoop-mapreduce-client-common.jar:/lib/hadoop-mapreduce/hadoop-mapreduce-client-jobclient.jar:/lib/hadoop-yarn/hadoop-yarn-client.jar:/lib/hadoop-yarn/hadoop-yarn-common.jar:/lib/hadoop-yarn/hadoop-yarn-api.jar:/lib/hadoop-yarn/lib/websocket-api-9.4.48.v20220622.jar:/lib/hadoop-yarn/lib/websocket-client-9.4.48.v20220622.jar:/lib/hadoop-yarn/lib/websocket-common-9.4.48.v20220622.jar"
hadoop:
  classpath:
    "/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/lib/*:/usr/local/hadoop/.//*:/usr/local/hadoop-hdfs/./:/usr/local/hadoop-hdfs/lib/*:/usr/local/hadoop-hdfs/.//*:/usr/local/hadoop-mapreduce/.//*:/usr/local/hadoop-yarn/./:/usr/local/hadoop-yarn/lib/*:/usr/local/hadoop-yarn/.//*"
config:
  lockFileDir: /var/run/datawave
  hadoopHome: /usr/local/hadoop
  mapredHome: /usr/local/hadoop-mapreduce
  accumuloUser: root
  accumuloPassword: ThisP@ssw0rd1sBANANAS
  accumuloDir: /opt/accumulo
  hdfsNamenode: hdfs://hdfs-nn:9000
  jobTracker: yarn-rm:8032
  zookeeper: zookeeper:2181
  instanceName: dev
  hadoopConfDif: /usr/local/hadoop/conf
  bulkDataTypes: shardStats
  liveDataTypes: wikipedia,mycsv,myjson,openlib
  sharedConfig:
    create: true
    claim: datawave-ingest-config-pvc
    path: /opt/datawave-ingest/current/shared-config
    size: 1Gi
    storageClass: "csi-hostpath-sc"

  files:
  - name: all-config.xml
    properties:
      - name: all.handler.classes
        value: >-
          datawave.ingest.mapreduce.handler.edge.ProtobufEdgeDataTypeHandler,datawave.ingest.mapreduce.handler.dateindex.DateIndexDataTypeHandler
        description: >-
          Datatype handlers to be utilized by all registered datatypes (in
          *addition* to any distinct handlers
                  that they may have set independently via their respective *-config.xml files)
      - name: all.ingest.helper.class
        value: ''
      - name: all.reader.class
        value: ''
      - name: all.data.category.marking.upstream.error
        value: SOMEDOMAIN=SOMEMARKINGASS
        description: >-
          If any record comes into the system with this security marking, then
          recognize it as an upstream error
                  which is considered to be FATAL in this system
      - description: Properties to include in the accumulo table config cache
        name: cache.table.properties
        value: >-
          table.file.compress.*,table.file.blocksize,table.file.replication,table.iterator.minc.*,crypto.*
      - name: all.filter.classes
        value: ${ALL_FILTER_CLASSES}
        description: >-
          This is the chain of context writers that will receive the output of
          all handlers and
                  higher prioriy content writers
      - name: all.filter.priority
        value: 50
        description: This is the priority of this context writer chain
      - name: all.data.combine.separator
        value: ':'
        description: Output separator for Virtual Fields
      - name: all.ingest.policy.enforcer.class
        value: datawave.policy.IngestPolicyEnforcer$NoOpIngestPolicyEnforcer
        description: |-
          Name of the class to use for policy enforcement.
                  (1) datawave.policy.IngestPolicyEnforcer$NoOpIngestPolicyEnforcer will assume all records are valid.
                  (2) datawave.policy.ExampleIngestPolicyEnforcer will perform some validations that you'd probably want
                      to enforce for all data in a production deployment
      - name: all.date.index.type.to.field.map
        value: LOADED=LOAD_DATE,ACTIVITY=EVENT_DATE
  - name: edge-ingest-config.xml
    properties:
    - name: protobufedge.table.name
      value: datawave.edge
    - name: protobufedge.table.loader.priority
      value: 30
    - name: datawave.edge.table.config.class
      value: datawave.ingest.table.config.ProtobufEdgeTableConfigHelper
    - name: protobufedge.table.metadata.enable
      value: true
    - name: protobufedge.table.disallowlist.enable
      value: true
    - name: protobufedge.spring.config
      value: config/edge-definitions.xml
    - name: protobufedge.setup.default.failurepolicy
      value: FAIL_JOB
    - name: protobufedge.process.default.failurepolicy
      value: FAIL_JOB
    - name: protobufedge.valid.activitytime.future.delta
      value: "86400000"
    - name: protobufedge.valid.activitytime.past.delta
      value: "315360000000"
  - name: shard-ingest-config.xml
    properties:
    - name: num.shards
      value: 10
    - name: sharded.table.names
      value: datawave.shard,datawave.error_s
      description: Comma-separated list of tables that need to pull splits from accumulo
    - name: shard.table.name
      value: datawave.shard
    - name: shard.index.create.uids
      value: true
    - name: shard.table.loader.priority
      value: 30
    - name: datawave.shard.table.config.class
      value: datawave.ingest.table.config.ShardTableConfigHelper
    - name: shard.table.locality.groups
      value: fullcontent:d,termfrequency:tf
      description: >-
        The list of locality groups in the form groupname:columnfamily, comma
        separated
    - name: shard.global.index.table.name
      value: datawave.shardIndex
    - name: shard.global.index.table.loader.priority
      value: 30
    - name: datawave.shardIndex.table.config.class
      value: datawave.ingest.table.config.ShardTableConfigHelper
    - name: shard.global.rindex.table.name
      value: datawave.shardReverseIndex
    - name: shard.global.rindex.table.loader.priority
      value: 30
    - name: datawave.shardReverseIndex.table.config.class
      value: datawave.ingest.table.config.ShardTableConfigHelper
    - name: markings.setup.iterator.enabled
      value: false
    - name: markings.setup.iterator.config
      value: ''
    - name: shard.global.index.geo.field
      value: LAT_LONG
    - name: partitioner.category.shardedTables
      value: datawave.ingest.mapreduce.partition.BalancedShardPartitioner
    - name: partitioner.category.member.datawave.shard
      value: shardedTables
    - name: index.tables.keep.count.only.entries
      value: false
  - name: metadata-config.xml
    properties:
    - name: metadata.ingest.helper.class
      value: datawave.ingest.data.config.ingest.MetaDataIngestHelper
    - name: metadata.reader.class
      value: ''
    - name: metadata.handler.classes
      value: ''
    - name: metadata.table.name
      value: datawave.metadata
    - name: metadata.table.loader.priority
      value: 30
    - name: datawave.metadata.table.config.class
      value: datawave.ingest.table.config.MetadataTableConfigHelper
    - name: metadata.loaddates.enabled
      value: true
    - name: metadata.loaddates.table.name
      value: datawave.loadDates
    - name: metadata.loaddates.table.loader.priority
      value: 30
    - name: metadata.loaddates.table.locality.groups
      value: ''
      description: >-
        The list of locality groups.  Groups are comma separated.  Column
        families are semicolon separated, e.g.
        groupName1:colFamily1;colFamily2,groupName2:colFamily3
    - name: datawave.loadDates.table.config.class
      value: datawave.ingest.table.config.LoadDateTableConfigHelper
    - name: metadata.term.frequency.enabled
      value: true
  - name: facet-config.xml
    properties:
    - name: facet.table.name
      value: datawave.facets
    - name: facet.table.loader.priority
      value: 40
    - name: facet.metadata.table.name
      value: datawave.facetMetadata
    - name: facet.metadata.table.loader.priority
      value: 40
    - name: facet.hash.table.name
      value: datawave.facetHashes
    - name: facet.hash.table.loader.priority
      value: 40
  - name: error-ingest-config.xml
    properties:
    - name: error.table.loader.priority
      value: 30
    - name: error.shard.table.name
      value: datawave.error_s
    - name: error.shard.table.loader.priority
      value: 30
    - name: error.shard.global.index.table.name
      value: datawave.error_i
    - name: error.shard.global.index.table.loader.priority
      value: 30
    - name: error.shard.global.rindex.table.name
      value: datawave.error_r
    - name: error.shard.global.rindex.table.loader.priority
      value: 30
    - name: error.shard.enable.bloom.filters
      value: false
      description: >-
        Whether or not to add bloom filters on the index tables. Default is
        false
    - name: error.shard.table.locality.groups
      value: fullcontent:d
      description: >-
        The list of locality groups in the form groupname:columnfamily, comma
        separated
    - name: error.metadata.table.name
      value: datawave.error_m
    - name: error.metadata.table.loader.priority
      value: 30
    - name: error.metadata.term.frequency.enabled
      value: true
    - name: datawave.error_s.table.config.class
      value: datawave.ingest.table.config.ErrorShardTableConfigHelper
    - name: datawave.error_i.table.config.class
      value: datawave.ingest.table.config.ErrorShardTableConfigHelper
    - name: datawave.error_r.table.config.class
      value: datawave.ingest.table.config.ErrorShardTableConfigHelper
    - name: datawave.error_m.table.config.class
      value: datawave.ingest.table.config.ErrorMetadataTableConfigHelper
    - name: error.table.ageoff.ttl
      value: 30
    - name: error.data.default.type.class
      value: datawave.data.type.LcNoDiacriticsType
    - name: error.table.ageoff.ttlunits
      value: d
    - name: error.use.default.type.handlers
      value: true
    - name: error.ingest.helper.class
      value: datawave.ingest.data.config.ingest.ErrorShardedIngestHelper
    - name: error.reader.class
      value: ''
    - name: error.handler.classes
      value: >-
        datawave.ingest.mapreduce.handler.error.ErrorShardedDataTypeHandler,${ADDITIONAL_ERROR_HANDLER_CLASSES}
    - name: error.data.category.marking.default
      value: PRIVATE
      description: >-
        Default ColumnVisibility to be applied to fields/records if none
        provided in the data
    - name: error.data.replace.malformed.utf8
      value: true
    - name: partitioner.category.member.datawave.error_s
      value: shardedTables
  - name : dateindex-ingest-config.xml
    properties:
      - name: date.index.table.name
        value: datawave.dateIndex
      - name: date.index.table.loader.priority
        value: 30
      - name: datawave.dateIndex.table.config.class
        value: datawave.ingest.table.config.DateIndexTableConfigHelper
      - name: date.index.table.locality.groups
        value: activity:ACTIVITY,loaded:LOADED
        description: >-
          The list of locality groups in the form groupname:columnfamily, comma separated
      - name: partitioner.dedicated.datawave.dateIndex
        value: datawave.ingest.mapreduce.partition.LimitedKeyPartitioner
      - name: date.index.num.shards
        value: 10
      - name: datawave.dateIndex.partition.limiter.max
        value: 3

  overallFlagConfig:
    defaultCfg:
      live:
        maxFlags: 4
        reducers: 10
        script: bin/ingest/live-ingest.sh
        fileListMarker: "***FILE_LIST***"
        collectMetrics: false
      bulk:
        maxFlags: 4
        reducers: 10
        script: bin/ingest/bulk-ingest.sh
        fileListMarker: "***FILE_LIST***"
        collectMetrics: false
    properties:
      live:
        timeoutMilliSecs: 10000
        baseHDFSDir: /data
        distributorType: simple
        filePattern: "[0-9a-zA-Z]*[0-9a-zA-Z]"
        hdfs: hdfs://hdfs-nn:9000
        socketPort: 20000
        datawaveHome: /opt/datawave-ingest/current
        flagFileDirectory: /srv/data/datawave/flags
        setFlagFileTimestamp: false
        useFolderTimestamp: false
      bulk:
        sleepMilliSecs: 5000
        timeoutMilliSecs: 480000
        baseHDFSDir: /data
        distributorType: simple
        filePattern: "[0-9a-zA-Z]*[0-9a-zA-Z]"
        hdfs: hdfs://hdfs-nn:9000
        socketPort: 20001
        datawaveHome: /opt/datawave-ingest/current
        flagFileDirectory: /srv/data/datawave/flags
        setFlagFileTimestamp: false
        useFolderTimestamp: false
        
  types:
    - name: openlib
      flagMakerConfig:
        liveFolder: openlib
        bulkFolder: openlib-bulk
        config:
          distrubutionArgs: none
          extraIngestArgs: "-data.name.override=openlib"
          inputFormat: datawave.ingest.json.mr.input.JsonInputFormat
          lifo: false
      properties: 
        "file.input.format": datawave.ingest.json.mr.input.JsonInputFormat
        "data.name": openlib
        "openlib.output.name": openlib
        "openlib.ingest.helper.class": datawave.ingest.json.config.helper.JsonIngestHelper
        "openlib.reader.class": datawave.ingest.json.mr.input.JsonRecordReader
        "openlib.handler.classes": "datawave.ingest.json.mr.handler.ContentJsonColumnBasedHandler,datawave.ingest.mapreduce.handler.facet.FacetHandler"
        "openlib.data.category.uuid.fields": KEY
        "openlib.data.separator": ","
        "openlib.data.header": KEY,TYPE,LASTMODIFIED_VALUE.LASTMODIFIED_0.VALUE_0
        "openlib.data.process.extra.fields": true
        "openlib.data.json.flattener.mode": GROUPED_AND_NORMAL
        "openlib.data.category.marking.default": PRIVATE|(BAR&amp;FOO)
        "openlib.data.category.date": LASTMODIFIED_VALUE.LASTMODIFIED_0.VALUE_0
        "openlib.data.category.date.formats": yyyy-MM-dd,yyyy-MM-dd'T'HH:mm:ss'Z',yyyy-MM-dd HH:mm:ss
        "openlib.data.category.index": KEY,NAME,TITLE,SUBTITLE,TYPE,ISBN_10
        "openlib.data.category.index.reverse": KEY,NAME,TITLE,SUBTITLE,TYPE,ISBN_10
        "openlib.data.category.token.fieldname.designator": _TOKEN
        "openlib.data.category.index.tokenize.allowlist": KEY,TITLE,SUBTITLE
        "openlib.data.category.index.only": KEY_TOKEN,TITLE_TOKEN,SUBTITLE_TOKEN
        "openlib.data.default.normalization.failure.policy": FAIL
        "openlib.data.default.type.class": datawave.data.type.LcNoDiacriticsType
        "openlib.LASTMODIFIED_VALUE.LASTMODIFIED_0.VALUE_0.data.field.type.class": datawave.data.type.DateType
        "openlib.facet.category.name.network": ""  
    - name: myjson
      flagMakerConfig:
        liveFolder: myjson
        bulkFolder: myjson-bulk
        config:
          distrubutionArgs: none
          extraIngestArgs: "-data.name.override=myjson"
          inputFormat: datawave.ingest.json.mr.input.JsonInputFormat
          lifo: false
      properties: 
        "file.input.format": datawave.ingest.json.mr.input.JsonInputFormat
        "data.name": myjson
        "myjson.output.name": tvmaze
        "myjson.ingest.helper.class": datawave.ingest.json.config.helper.JsonIngestHelper
        "myjson.reader.class": datawave.ingest.json.mr.input.JsonRecordReader
        "myjson.handler.classes": "datawave.ingest.json.mr.handler.ContentJsonColumnBasedHandler,datawave.ingest.mapreduce.handler.facet.FacetHandler"
        "myjson.data.category.uuid.fields": ID,EXTERNALS_THETVDB,EXTERNALS_TVRAGE,EXTERNALS_IMDB
        "myjson.data.separator": ","
        "myjson.data.header": ID,NAME,PREMIERED,RUNTIME,STATUS,SUMMARY,OFFICIALSITE,LANGUAGE,GENRES,WEIGHT,URL,TYPE
        "myjson.data.process.extra.fields": true
        "myjson.data.json.flattener.mode": GROUPED_AND_NORMAL
        "myjson.data.category.marking.default": PRIVATE|(BAR&amp;FOO)
        "myjson.SUMMARY.data.field.marking": PUBLIC
        "myjson.data.category.date": PREMIERED
        "myjson.data.category.date.formats": yyyy-MM-dd,yyyy-MM-dd'T'HH:mm:ss'Z',yyyy-MM-dd HH:mm:ss
        "myjson.data.category.index": NAME,ID,ID,EXTERNALS_THETVDB,EXTERNALS_TVRAGE,EXTERNALS_IMDB,EMBEDDED_CAST_CHARACTER_NAME,EMBEDDED_CAST_PERSON_NAME,EMBEDDED_CAST_PERSON_ID,GENRES,NETWORK_NAME,OFFICIALSITE,TYPE,STATUS,RUNTIME,URL
        "myjson.data.category.index.reverse": NAME,NETWORK_NAME,OFFICIALSITE,URL
        "myjson.data.category.token.fieldname.designator": _TOKEN
        "myjson.data.category.index.tokenize.allowlist": SUMMARY,NETWORK_NAME,NAME,EMBEDDED_CAST_CHARACTER_NAME,EMBEDDED_CAST_PERSON_NAME
        "myjson.data.category.index.only": SUMMARY_TOKEN,NETWORK_NAME_TOKEN,NAME_TOKEN,EMBEDDED_CAST_CHARACTER_NAME_TOKEN,EMBEDDED_CAST_PERSON_NAME_TOKEN
        "myjson.data.default.normalization.failure.policy": FAIL
        "myjson.data.default.type.class": datawave.data.type.LcNoDiacriticsType
        "myjson.PREMIERED.data.field.type.class": datawave.data.type.DateType
        "myjson.WEIGHT.data.field.type.class": datawave.data.type.NumberType
        "myjson.RUNTIME.data.field.type.class": datawave.data.type.NumberType
        "myjson.facet.category.name.network": "NETWORK_NAME;GENRES,EMBEDDED_CAST_PERSON_GENDER,RATING_AVERAGE"  
    - name: mycsv
      flagMakerConfig:
        liveFolder: mycsv
        bulkFolder: mycsv-bulk
        config:
          distrubutionArgs: none
          extraIngestArgs: "-data.name.override=mycsv"
          inputFormat: datawave.ingest.csv.mr.input.CSVFileInputFormat
          lifo: false
      properties: 
        "file.input.format": datawave.ingest.csv.mr.input.CSVFileInputFormat
        "data.name": mycsv
        "mycsv.output.name": csv
        "mycsv.ingest.helper.class": datawave.ingest.json.config.helper.JsonIngestHelper
        "mycsv.reader.class": datawave.ingest.json.mr.input.JsonRecordReader
        "mycsv.handler.classes": "datawave.ingest.json.mr.handler.ContentJsonColumnBasedHandler,datawave.ingest.mapreduce.handler.facet.FacetHandler"
        "mycsv.data.category.uuid.fields": ID,EXTERNALS_THETVDB,EXTERNALS_TVRAGE,EXTERNALS_IMDB
        "mycsv.data.separator": ","
        "mycsv.data.header": ID,NAME,PREMIERED,RUNTIME,STATUS,SUMMARY,OFFICIALSITE,LANGUAGE,GENRES,WEIGHT,URL,TYPE
        "mycsv.data.process.extra.fields": true
        "mycsv.data.json.flattener.mode": GROUPED_AND_NORMAL"
        "mycsv.data.category.marking.default": PRIVATE|(BAR&amp;FOO)
        "mycsv.SUMMARY.data.field.marking": PUBLIC
        "mycsv.data.category.date.formats": yyyy-MM-dd,yyyy-MM-dd'T'HH:mm:ss'Z',yyyy-MM-dd HH:mm:ss
        "mycsv.data.category.index": NAME,ID,ID,EXTERNALS_THETVDB,EXTERNALS_TVRAGE,EXTERNALS_IMDB,EMBEDDED_CAST_CHARACTER_NAME,EMBEDDED_CAST_PERSON_NAME,EMBEDDED_CAST_PERSON_ID,GENRES,NETWORK_NAME,OFFICIALSITE,TYPE,STATUS,RUNTIME,URL
        "mycsv.data.category.index.reverse": NAME,NETWORK_NAME,OFFICIALSITE,URL
        "mycsv.data.category.token.fieldname.designator": _TOKEN
        "mycsv.data.category.index.tokenize.allowlist": SUMMARY,NETWORK_NAME,NAME,EMBEDDED_CAST_CHARACTER_NAME,EMBEDDED_CAST_PERSON_NAME
        "mycsv.data.category.index.only": SUMMARY_TOKEN,NETWORK_NAME_TOKEN,NAME_TOKEN,EMBEDDED_CAST_CHARACTER_NAME_TOKEN,EMBEDDED_CAST_PERSON_NAME_TOKEN
        "mycsv.data.default.normalization.failure.policy": FAIL
        "mycsv.data.default.type.class": datawave.data.type.LcNoDiacriticsType
        "mycsv.PREMIERED.data.field.type.class": datawave.data.type.DateType
        "mycsv.WEIGHT.data.field.type.class": datawave.data.type.NumberType
        "mycsv.RUNTIME.data.field.type.class": datawave.data.type.NumberType
        "mycsv.facet.category.name.network": "NETWORK_NAME;GENRES,EMBEDDED_CAST_PERSON_GENDER,RATING_AVERAGE"    