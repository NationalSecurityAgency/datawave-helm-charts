apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Values.global.hadoopConfigMap }}
  labels:
    app: {{ include "hadoop.appName" . }}
    chart: {{ include "hadoop.chart" . }}
    release: {{ .Release.Name }}
data:
  bootstrap.sh: |
    #!/bin/bash

    : ${HADOOP_HOME:=/usr/local/hadoop}

    : ${HADOOP_PREFIX:=/usr/local/hadoop}

    . $HADOOP_HOME/etc/hadoop/hadoop-env.sh

    # Directory to find config artifacts
    CONFIG_DIR="/tmp/hadoop-config"

    # Copy config files from volume mount

    for f in slaves core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml erasure-coding-policies.xml; do
      if [[ -e ${CONFIG_DIR}/$f ]]; then
        cp ${CONFIG_DIR}/$f $HADOOP_HOME/etc/hadoop/$f
      else
        echo "ERROR: Could not find $f in $CONFIG_DIR"
        exit 1
      fi
    done

    # installing libraries if any - (resource urls added comma separated to the ACP system variable)
    cd $HADOOP_HOME/ ; for cp in ${ACP//,/ }; do  echo == $cp; curl -LO $cp ; done; cd -

    if [[ "${HOSTNAME}" =~ "hdfs-nn" ]]; then
      echo "Check if Namenode directory exists"
      if [[ ! -d /usr/local/hadoop/data ]]; then
        echo "Creating Namenode directory"
        mkdir -p /usr/local/hadoop/data/
      else
        echo "/usr/local/hadoop/data/ exists"
      fi
      if [[ ! -f /usr/local/hadoop/data/current/VERSION ]]; then
        echo "Formatting..."
        $HADOOP_HDFS_HOME/bin/hdfs namenode -format -force -nonInteractive
      else
        echo "/usr/local/hadoop/data/ exists"
        echo $(ls /usr/local/hadoop/data/)
        echo $(ls /usr/local/hadoop/data/current)
      fi
      echo "END IFS"
      echo "Start NameNode"
      $HADOOP_PREFIX/sbin/hadoop-daemon.sh start namenode
    fi

    if [[ "${HOSTNAME}" =~ "hdfs-dn" ]]; then
      echo "Check if Datanode directory exists"
      if [[ ! -d /usr/local/hadoop/hdfs/datanode ]]; then
        echo "Creating Datanode directory"
        mkdir -p /usr/local/hadoop/hdfs/datanode
      fi
      #  wait up to 30 seconds for namenode
      (while [[ $count -lt 15 && -z `curl -sf http://hdfs-nn:9870/dfshealth.html` ]]; do ((count=count+1)) ; echo "Waiting for hdfs-nn" ; sleep 2; done && [[ $count -lt 15 ]])
      [[ $? -ne 0 ]] && echo "Timeout waiting for hdfs-nn, exiting." && exit 1

      $HADOOP_HOME/sbin/hadoop-daemon.sh start datanode
      $HADOOP_HDFS_HOME/bin/hdfs dfs -chmod 777 /
    fi

    if [[ "${HOSTNAME}" =~ "yarn-rm" ]]; then
      (while [[ $count -lt 15 && -z `curl -sf http://hdfs-nn:9870/dfshealth.html` ]]; do ((count=count+1)) ; echo "Waiting for hdfs-nn" ; sleep 2; done && [[ $count -lt 15 ]])
      [[ $? -ne 0 ]] && echo "Timeout waiting for hdfs-nn, exiting." && exit 1
      cp ${CONFIG_DIR}/start-yarn-rm.sh $HADOOP_YARN_HOME/sbin/
      cd $HADOOP_YARN_HOME/sbin
      chmod +x start-yarn-rm.sh
      ./start-yarn-rm.sh
    fi

    if [[ "${HOSTNAME}" =~ "yarn-nm" ]]; then
      sed -i '/<\/configuration>/d' $HADOOP_HOME/etc/hadoop/yarn-site.xml
      cat >> $HADOOP_HOME/etc/hadoop/yarn-site.xml <<- EOM
      <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>${MY_MEM_LIMIT:-4096}</value>
      </property>

      <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>${MY_CPU_LIMIT:-2}</value>
      </property>
    EOM
      echo '</configuration>' >> $HADOOP_HOME/etc/hadoop/yarn-site.xml
      cp ${CONFIG_DIR}/start-yarn-nm.sh $HADOOP_YARN_HOME/sbin/
      cd $HADOOP_YARN_HOME/sbin
      chmod +x start-yarn-nm.sh

      #  wait up to 30 seconds for resourcemanager
      (while [[ $count -lt 15 && -z `curl -sf http://yarn-rm:8088/ws/v1/cluster/info` ]]; do ((count=count+1)) ; echo "Waiting for yarn-rm" ; sleep 2; done && [[ $count -lt 15 ]])
      [[ $? -ne 0 ]] && echo "Timeout waiting for yarn-rm, exiting." && exit 1

      ./start-yarn-nm.sh
    fi

    if [[ $1 == "-d" ]]; then
      until find ${HADOOP_HOME}/logs -mmin -1 | egrep -q '.*'; echo "`date`: Waiting for logs..." ; do sleep 2 ; done
      tail -F ${HADOOP_HOME}/logs/* &
      while true; do sleep 1000; done
    fi

    if [[ $1 == "-bash" ]]; then
      /bin/bash
    fi

  log4j.properties: |
    # Define some default values that can be overridden by system properties
    hadoop.root.logger=INFO,console
    hadoop.log.dir=.
    hadoop.log.file=hadoop.log
    
    # Define the root logger to the system property "hadoop.root.logger".
    log4j.rootLogger=${hadoop.root.logger}
    
    # Logging Threshold
    log4j.threshhold=ALL
    
    #
    # Daily Rolling File Appender
    #
    
    log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender
    log4j.appender.DRFA.File=${hadoop.log.dir}/${hadoop.log.file}
    log4j.appender.DRFA.DatePattern=.yyyy-MM-dd
    
    # 30-day backup
    #log4j.appender.DRFA.MaxBackupIndex=30
    log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout
    
    # Pattern format: Date LogLevel LoggerName LogMessage
    log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
    # Debugging Pattern format
    #log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
    
    #
    # console
    # Add "console" to rootlogger above if you want to use this 
    #
    
    log4j.appender.console=org.apache.log4j.ConsoleAppender
    log4j.appender.console.target=System.err
    log4j.appender.console.layout=org.apache.log4j.PatternLayout
    log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n
    
    #
    # TaskLog Appender
    #
    
    #Default values
    hadoop.tasklog.taskid=null
    hadoop.tasklog.noKeepSplits=4
    hadoop.tasklog.totalLogFileSize=100
    hadoop.tasklog.purgeLogSplits=true
    hadoop.tasklog.logsRetainHours=12
    hadoop.tasklog.iscleanup=false
    
    log4j.appender.TLA=org.apache.hadoop.mapred.TaskLogAppender
    log4j.appender.TLA.taskId=${hadoop.tasklog.taskid}
    log4j.appender.TLA.totalLogFileSize=${hadoop.tasklog.totalLogFileSize}
    
    log4j.appender.TLA.isCleanup=${hadoop.tasklog.iscleanup}
    
    log4j.appender.TLA.layout=org.apache.log4j.PatternLayout
    log4j.appender.TLA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n

    
    #=======
    # security audit logging
    
    security.audit.logger=INFO,console
    log4j.category.SecurityLogger=${security.audit.logger}
    log4j.additivity.SecurityLogger=false
    log4j.appender.DRFAS=org.apache.log4j.DailyRollingFileAppender 
    log4j.appender.DRFAS.File=/var/local/hadoop/logs/${hadoop.id.str}/${hadoop.id.str}-auth.log
    log4j.appender.D RFAS.layout=org.apache.log4j.PatternLayout
    log4j.appender.DRFAS.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
    log4j.appender.DRFAS.DatePattern=.yyyy-MM-dd
    
    # hdfs audit logging 
    
    hdfs.audit.logger=INFO,console
    log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=${hdfs.audit.logger}
    log4j.additivity.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=false
    log4j.appender.DRFAAUDIT=org.apache.log4j.DailyRollingFileAppender
    log4j.appender.DRFAAUDIT.File=/var/local/hadoop/logs/hadoop-logs/hdfs-audit.log
    log4j.appender.DRFAAUDIT.layout=org.apache.log4j.PatternLayout
    log4j.appender.DRFAAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
    log4j.appender.DRFAAUDIT.DatePattern=.yyyy-MM-dd
    
    
    # mapred audit logging
    
    mapred.audit.logger=INFO,console
    log4j.logger.org.apache.hadoop.mapred.AuditLogger=${mapred.audit.logger}
    log4j.additivity.org.apache.hadoop.mapred.AuditLogger=false
    log4j.appender.MRAUDIT=org.apache.log4j.DailyRollingFileAppender
    log4j.appender.MRAUDIT.File=/var/local/hadoop/logs/hadoop-logs/mapred-audit.log
    log4j.appender.MRAUDIT.layout=org.apache.log4j.PatternLayout
    log4j.appender.MRAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
    log4j.appender.MRAUDIT.DatePattern=.yyyy-MM-dd
    
    
    # Mapred job summary 
    
    mapred.jobsummary.logger=INFO,console
    log4j.logger.org.apache.hadoop.mapred.JobInProgress$JobSummary=${mapred.jobsummary.logger}
    log4j.additivity.org.apache.hadoop.mapred.JobInProgress$JobSummary=false
    log4j.appender.JSA=org.apache.log4j.DailyRollingFileAppender
    log4j.appender.JSA.File=${hadoop.log.dir}/mapred-jobsummary.log
    log4j.appender.JSA.layout=org.apache.log4j.PatternLayout
    log4j.appender.JSA.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
    log4j.appender.JSA.DatePattern=.yyyy-MM-dd
    {{- with .Values.config.loggingProperties }}
    {{- range . }}
    {{ .name }}={{ .value }}
    {{- end }}
    {{- end }}
  core-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
      {{- with .Values.config.coreSiteProperties }}
      {{- range . }}
      <property>
          <name>{{ .name }}</name>
          <value>{{ .value }}</value>
          <description>{{ .description }}</description>
      </property>
      {{- end }}
      {{- end }}
    </configuration>

  hdfs-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
      {{- with .Values.config.hdfsSiteProperties }}
      {{- range . }}
      <property>
          <name>{{ .name }}</name>
          <value>{{ .value }}</value>
      </property>
      {{- end }}
      {{- end }}

    </configuration>

  mapred-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

    <configuration>
      {{- with .Values.config.mapredSiteProperties }}
      {{- range . }}
      <property>
          <name>{{ .name }}</name>
          <value>{{ .value }}</value>
      </property>
      {{- end }}
      {{- end }}
    </configuration>

  slaves: |
    localhost

  start-yarn-nm.sh: |
    #!/usr/bin/env bash

    # Licensed to the Apache Software Foundation (ASF) under one or more
    # contributor license agreements.  See the NOTICE file distributed with
    # this work for additional information regarding copyright ownership.
    # The ASF licenses this file to You under the Apache License, Version 2.0
    # (the "License"); you may not use this file except in compliance with
    # the License.  You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.


    # Start all yarn daemons.  Run this on manager node.

    echo "starting yarn daemons"

    bin=`dirname "${BASH_SOURCE-$0}"`
    bin=`cd "$bin"; pwd`

    DEFAULT_LIBEXEC_DIR="$bin"/../libexec
    HADOOP_LIBEXEC_DIR=${HADOOP_LIBEXEC_DIR:-$DEFAULT_LIBEXEC_DIR}
    . $HADOOP_LIBEXEC_DIR/yarn-config.sh

    # start resourceManager
    # "$bin"/yarn-daemon.sh --config $YARN_CONF_DIR  start resourcemanager
    # start nodeManager
    "$bin"/yarn-daemon.sh --config $YARN_CONF_DIR  start nodemanager
    # start proxyserver
    #"$bin"/yarn-daemon.sh --config $YARN_CONF_DIR  start proxyserver

  start-yarn-rm.sh: |
    #!/usr/bin/env bash

    # Licensed to the Apache Software Foundation (ASF) under one or more
    # contributor license agreements.  See the NOTICE file distributed with
    # this work for additional information regarding copyright ownership.
    # The ASF licenses this file to You under the Apache License, Version 2.0
    # (the "License"); you may not use this file except in compliance with
    # the License.  You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.


    # Start all yarn daemons.  Run this on manager node.

    echo "starting yarn daemons"

    bin=`dirname "${BASH_SOURCE-$0}"`
    bin=`cd "$bin"; pwd`

    DEFAULT_LIBEXEC_DIR="$bin"/../libexec
    HADOOP_LIBEXEC_DIR=${HADOOP_LIBEXEC_DIR:-$DEFAULT_LIBEXEC_DIR}
    . $HADOOP_LIBEXEC_DIR/yarn-config.sh

    # start resourceManager
    "$bin"/yarn-daemon.sh --config $YARN_CONF_DIR  start resourcemanager
    # start nodeManager
    # "$bin"/yarn-daemons.sh --config $YARN_CONF_DIR  start nodemanager
    # start proxyserver
    "$bin"/yarn-daemon.sh --config $YARN_CONF_DIR  start proxyserver

    sleep 60
    # start history server
    "$bin"/../bin/mapred --config $HADOOP_CONF_DIR  --daemon start historyserver

  yarn-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

    <configuration>
      {{- with .Values.config.yarnSiteProperties }}
      {{- range . }}
      <property>
          <name>{{ .name }}</name>
          <value>{{ .value }}</value>
      </property>
      {{- end }}
      {{- end }}
    </configuration>

  fair-scheduler.xml: |
      <?xml version="1.0"?>
      <allocations>
        <queue name="liveIngestQueue">
          <minResources>1 mb, 1 vcores</minResources>
          <maxResources>90000 mb,0vcores</maxResources>
          <maxRunningApps>50</maxRunningApps>
          <maxAMShare>0.5</maxAMShare>
          <weight>2.0</weight>
          <schedulingPolicy>fair</schedulingPolicy
        </queue>
  
        <queueMaxAMShareDefault>0.5</queueMaxAMShareDefault>
        <queueMaxResourcesDefault>40000 mb,0vcores</queueMaxResourcesDefault>
        
        <queue name="bulkIngestQueue" type="parent">
        <weight>3.0</weight>
        <maxChildResources>4096 mb,4vcores</maxChildResources>
        </queue>
  
        <queuePlacementPolicy>
          <rule name="default" queue="liveIngestQueue"/>
        </queuePlacementPolicy>
      </allocations>
  erasure-coding-policies.xml: |
    <?xml version="1.0"?>

    <!--
     Licensed to the Apache Software Foundation (ASF) under one
     or more contributor license agreements.  See the NOTICE file
     distributed with this work for additional information
     regarding copyright ownership.  The ASF licenses this file
     to you under the Apache License, Version 2.0 (the
     "License"); you may not use this file except in compliance
     with the License.  You may obtain a copy of the License at
    
         http://www.apache.org/licenses/LICENSE-2.0
    
     Unless required by applicable law or agreed to in writing, software
     distributed under the License is distributed on an "AS IS" BASIS,
     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
     See the License for the specific language governing permissions and
     limitations under the License.
    -->
    
    <!--
        This is the template for user-defined EC policies configuration.
        All policies and schemas are defined within the 'configuration' tag
        which is the top level element for this XML document. The 'layoutversion'
        tag contains the version of EC policy XML file format, and user-defined EC
        schemas are included within the 'schemas' tag. The 'policies' tag
        contains all the user defined EC policies, and each policy consists of
        schema id and cellsize.
    -->
    <configuration>
    <!-- The version of EC policy XML file format, it must be an integer -->
    <layoutversion>1</layoutversion>
    <schemas>
      <!-- schema id is only used to reference internally in this document -->
      <schema id="RS-6-3">
        <codec>rs</codec>
        <k>6</k>
        <m>3</m>
        <options> </options>
      </schema>
      <schema id="RS-6-2">
        <codec>rs</codec>
        <k>6</k>
        <m>2</m>
        <options> </options>
      </schema>
      <schema id="RS-3-2">
        <codec>rs</codec>
        <k>3</k>
        <m>2</m>
        <options> </options>
      </schema>
     <schema id="RS-4-2">
        <codec>rs</codec>
        <k>4</k>
        <m>2</m>
        <options> </options>
      </schema>
    <schema id="RS-4-3">
      <codec>rs</codec>
      <k>4</k>
      <m>3</m>
    </schema>
    </schemas>
    <policies>
      <policy>
        <!-- the combination of schema ID and cellsize(in unit k) defines a unique
         policy, for example 'xor-2-1-256k', case insensitive -->
        <!-- schema is referred by its id -->
        <schema>RS-4-3</schema>
        <!-- cellsize must be an positive integer multiple of 1024(1k) -->
        <!-- maximum cellsize is defined by 'dfs.namenode.ec.policies.max.cellsize' property -->
        <cellsize>131072</cellsize>
      </policy>
    <policy>
     <schema>RS-3-2</schema>
        <!-- cellsize must be an positive integer multiple of 1024(1k) -->
        <!-- maximum cellsize is defined by 'dfs.namenode.ec.policies.max.cellsize' property -->
        <cellsize>131072</cellsize>  
    </policy>
    <policy>
     <schema>RS-4-2</schema>
        <!-- cellsize must be an positive integer multiple of 1024(1k) -->
        <!-- maximum cellsize is defined by 'dfs.namenode.ec.policies.max.cellsize' property -->
        <cellsize>131072</cellsize>
    </policy>
    <policy>
     <schema>RS-6-3</schema>
        <!-- cellsize must be an positive integer multiple of 1024(1k) -->
        <!-- maximum cellsize is defined by 'dfs.namenode.ec.policies.max.cellsize' property -->
        <cellsize>131072</cellsize>
    </policy>
    <policy>
     <schema>RS-6-2</schema>
        <!-- cellsize must be an positive integer multiple of 1024(1k) -->
        <!-- maximum cellsize is defined by 'dfs.namenode.ec.policies.max.cellsize' property -->
        <cellsize>65536</cellsize>
    </policy>
    <policy>
     <schema>RS-6-2</schema>
        <!-- cellsize must be an positive integer multiple of 1024(1k) -->
        <!-- maximum cellsize is defined by 'dfs.namenode.ec.policies.max.cellsize' property -->
        <cellsize>131072</cellsize>
    </policy>
    <policy>
        <schema>RS-6-3</schema>
        <cellsize>65536</cellsize>
      </policy>
       <policy>
        <schema>RS-4-2</schema>
        <cellsize>65536</cellsize>
      </policy>
     <policy>
        <schema>RS-4-3</schema>
        <cellsize>65536</cellsize>
      </policy>
     <policy>
        <schema>RS-3-2</schema>
        <cellsize>65536</cellsize>
      </policy>
    </policies>
    </configuration>
