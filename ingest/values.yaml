global:
  pullSecrets:
    - dockerconfigjson-ghcr
  volumes:
    hadoop:
      source:
        name: hadoop-config
    accumulo:
      source:
        name: accumulo-config
nameOverride: ""
fullnameOverride: ""
labels: {}
image:
  repository: ghcr.io/nationalsecurityagency/datawave/ingest-kubernetes
  tag: 7.32.2
  pullPolicy: IfNotPresent
deployment:
  replicaCount: 1
  resources: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}
libs: "/lib/hadoop-mapreduce/hadoop-mapreduce-client-core.jar:/lib/hadoop-hdfs/hadoop-hdfs-client.jar:/lib/hadoop-mapreduce/hadoop-mapreduce-client-common.jar:/lib/hadoop-mapreduce/hadoop-mapreduce-client-jobclient.jar:/lib/hadoop-mapreduce/hadoop-mapreduce-client-common.jar:/lib/hadoop-mapreduce/hadoop-mapreduce-client-jobclient.jar:/lib/hadoop-yarn/hadoop-yarn-client.jar:/lib/hadoop-yarn/hadoop-yarn-common.jar:/lib/hadoop-yarn/hadoop-yarn-api.jar:/lib/hadoop-yarn/lib/websocket-api-9.4.48.v20220622.jar:/lib/hadoop-yarn/lib/websocket-client-9.4.48.v20220622.jar:/lib/hadoop-yarn/lib/websocket-common-9.4.48.v20220622.jar"
hadoop:
  classpath:
    "/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/lib/*:/usr/local/hadoop/.//*:/usr/local/hadoop-hdfs/./:/usr/local/hadoop-hdfs/lib/*:/usr/local/hadoop-hdfs/.//*:/usr/local/hadoop-mapreduce/.//*:/usr/local/hadoop-yarn/./:/usr/local/hadoop-yarn/lib/*:/usr/local/hadoop-yarn/.//*"
config:
  lockFileDir: /var/run/datawave
  hadoopHome: /usr/local/hadoop
  mapredHome: /usr/local/hadoop-mapreduce
  accumuloUser: root
  accumuloPassword: ThisP@ssw0rd1sBANANAS
  accumuloDir: /opt/accumulo
  hdfsNamenode: hdfs://hdfs-nn:9000
  jobTracker: yarn-rm:8032
  zookeeper: zookeeper:2181
  instanceName: dev
  hadoopConfDif: /usr/local/hadoop/conf
  bulkDataTypes: shardStats
  liveDataTypes: wikipedia,mycsv,myjson,openlib
  sharedConfig:
    create: true
    claim: datawave-ingest-config-pvc
    path: /opt/datawave-ingest/current/shared-config
    size: 1Gi
    storageClass: "csi-hostpath-sc"
  initIngestScript: |-
    echo "Create datawave tables"
    /opt/datawave-ingest/current/bin/ingest/create-all-tables.sh 
    
    echo "Seeding Splits"
    
    echo "Create Query Metrics S table"
    /opt/accumulo/bin/accumulo shell -u root -p {{ .Values.config.accumuloPassword }} -e "createtable datawave.queryMetrics_s"
    
    echo "Create Splits"
    /opt/datawave-ingest/current/bin/ingest/create-shards-since.sh $(date --date='$10 days ago' +%y%m%d)
    echo "Seed Index Splits"
    /opt/datawave-ingest/current/bin/ingest/seed-index-splits.sh
    
    echo "loading job cache"
    /opt/datawave-ingest/current/bin/ingest/load-job-cache.sh
    
    echo "Generate Splits File"
    /opt/datawave-ingest/current/bin/ingest/generate-splits-file.sh
    
    {{- if (.Values.config.sharedConfig).claim }}
    echo "synchronizing shared config to pvc {{ .Values.config.sharedConfig.claim }} at {{ .Values.config.sharedConfig.path }}"
    cp -Lr /opt/datawave-ingest/current/config/* {{ .Values.config.sharedConfig.path }}
    {{- end }}
      
    echo "Starting datawave ingest"
    /opt/datawave-ingest/current/bin/system/start-all.sh
  ingestPasswd: |-
    export PASSWORD=ThisP@ssw0rd1sBANANAS
    export KEYSTORE_PASSWORD=root
    export TRUSTSTORE_PASSWORD=root
  ingestEnv: |-
    if [[ ! -d ${LOCK_FILE_DIR} || ! -w ${LOCK_FILE_DIR} ]]; then
      "Lock file directory ${LOCK_FILE_DIR} does not exist or is not writable. Exiting..."
      exit 1
    fi
    
    . ../system/header.sh
    
    . ../ingest/tables-env.sh
    
    # regex matching changed since bash 3.1....ensure we are forward compatible
    shopt -s compat31 > /dev/null 2>&1
    
    # load the external password specifications.  The following needs to be defined in this script: PASSWORD, TRUSTSTORE_PASSWORD, KEYSTORE_PASSWORD
    function checkForVar (){
       found=`cat $1 | egrep " $2 *="`
       if [[ "$found" == "" ]]; then
          echo "$2,"
       fi
    }
    
    PASSWORD_INGEST_ENV="/opt/datawave-ingest/ingest-passwd.sh"
    if [[ "$PASSWORD_INGEST_ENV" != "" ]]; then
       if [[ -e /opt/datawave-ingest/ingest-passwd.sh ]]; then
          missing=\
    "$(checkForVar $PASSWORD_INGEST_ENV "PASSWORD")\
    $(checkForVar $PASSWORD_INGEST_ENV "TRUSTSTORE_PASSWORD")\
    $(checkForVar $PASSWORD_INGEST_ENV "KEYSTORE_PASSWORD")"
          if [[ "$missing" != "" ]]; then
             echo "FATAL: /opt/datawave-ingest/ingest-passwd.sh is missing the following definitions: $missing"
             exit 10
          fi
          . "$PASSWORD_INGEST_ENV"
       else
          echo "FATAL: /opt/datawave-ingest/ingest-passwd.sh was not found  Please create that script on this system."
          exit 10
       fi
    else
       echo "FATAL: PASSWORD_INGEST_ENV was not defined.  Please define this in your deployment properties and create that script on this system."
       echo "   e.g. /opt/datawave-ingest/ingest-passwd.sh:"
       echo "        export PASSWORD=\"accumulo_passwd\""
       echo "        export TRUSTSTORE_PASSWORD=\"trust_passwd\""
       echo "        export KEYSTORE_PASSWORD=\"cert_passwd\""
       exit 10
    fi
    
    # if a deployment specific environment has been specified, then load it
    ADDITIONAL_INGEST_ENV="${ADDITIONAL_INGEST_ENV}"
    if [[ "$ADDITIONAL_INGEST_ENV" != "" ]]; then
       . "$ADDITIONAL_INGEST_ENV"
    fi
    
    ADDITIONAL_INGEST_LIBS="${ADDITIONAL_INGEST_LIBS}"
    
    # NOTE:
    # Make sure to use the following pattern when setting variables and you want to assign them default values.
    #
    # (1) VARIABLE_X=${VARIABLE_X}
    # (2) VARIABLE_X=${VARIABLE_X:-<default value>}
    #
    # - The first assignment is used by maven during compile to set variables to the value defined in the
    #   default.properties (or dev.properties, etc.) files.
    # - The second assignment sets a variable to a "default" value in the event that it was not set in the
    #   first (previous) assignment.
    #
    
    # Provides a method to run map-file-bulk-loader as a different user
    MAP_FILE_LOADER_COMMAND_PREFIX="${MAP_FILE_LOADER_COMMAND_PREFIX}"
    MAP_FILE_LOADER_EXTRA_ARGS=""
    MAP_FILE_LOADER_SEPARATE_START="${MAP_FILE_LOADER_SEPARATE_START}"
    MAP_FILE_LOADER_SEPARATE_START="${MAP_FILE_LOADER_SEPARATE_START:-false}"
    RCPT_TO="hadoop@localhost"
    SEND_JOB_EMAIL_DISABLED="${SEND_JOB_EMAIL_DISABLED}"
    
    WAREHOUSE_ACCUMULO_LIB=""
    WAREHOUSE_ACCUMULO_BIN=""
    WAREHOUSE_ACCUMULO_LIB="${WAREHOUSE_ACCUMULO_LIB:-$WAREHOUSE_ACCUMULO_HOME/lib}"
    WAREHOUSE_ACCUMULO_BIN="${WAREHOUSE_ACCUMULO_BIN:-$WAREHOUSE_ACCUMULO_HOME/bin}"
    
    WAREHOUSE_NAME_BASE_DIR="${WAREHOUSE_NAME_BASE_DIR}"
    
    # setting these two times may seem unnecessary, but the first one is required if
    # the property is set in the assembly properties (see datawave_deploy).  The second
    # one is needed if it is not set explicitly but HADOOP_HOME is.
    WAREHOUSE_HADOOP_HOME="${WAREHOUSE_HADOOP_HOME}"
    WAREHOUSE_HADOOP_HOME="${WAREHOUSE_HADOOP_HOME:-$HADOOP_HOME}"
    WAREHOUSE_MAPRED_HOME="${WAREHOUSE_MAPRED_HOME}"
    WAREHOUSE_MAPRED_HOME="${WAREHOUSE_MAPRED_HOME:-$MAPRED_HOME}"
    
    
    
    # setting these two times may seem unnecessary, but the first one is required if
    # the property is set in the assembly properties (see datawave_deploy).  The second
    # one is needed if it is not set explicitly but HADOOP_HOME is.
    INGEST_HADOOP_HOME="${INGEST_HADOOP_HOME}"
    INGEST_HADOOP_HOME="${INGEST_HADOOP_HOME:-$HADOOP_HOME}"
    INGEST_MAPRED_HOME="${INGEST_MAPRED_HOME}"
    INGEST_MAPRED_HOME="${INGEST_MAPRED_HOME:-$MAPRED_HOME}"
    
    # STAGING_HOSTS is a comma delimited list of hosts
    STAGING_HOSTS="`nodeattr -c staging`"
    INGEST_HOST="ingestmanager"
    ROLLUP_HOST="ingestmanager"
    
    # hadoop and child opts for ingest
    MAPRED_INGEST_OPTS="-useInlineCombiner"
    HADOOP_INGEST_OPTS=""
    CHILD_INGEST_OPTS=""
    
    LIVE_CHILD_MAX_MEMORY_MB="${LIVE_CHILD_MAX_MEMORY_MB}"
    BULK_CHILD_MAX_MEMORY_MB="${BULK_CHILD_MAX_MEMORY_MB}"
    MISSION_MGMT_CHILD_MAP_MAX_MEMORY_MB="${MISSION_MGMT_CHILD_MAP_MAX_MEMORY_MB}"
    
    # The next two comma delimited lists work in concert with each other and must align
    CONFIG_DATA_TYPES="${CONFIG_DATA_TYPES}"
    CONFIG_FILES="${CONFIG_FILES}"
    if [[ "$CONFIG_FILES" == "" ]]; then
        # attempt to create the CONFIG_DATA_TYPES and CONFIG_FILES by scanning the config directory
        for config_file in ../../config/*.xml; do
            CONFIG_DATA_TYPE=`grep -A 1 -B 1 '>data.name<' $config_file | grep '<value>' | sed 's/.*<value>//' | sed 's/<\/value>.*//' | sed 's/\.//'`
            if [[ "$CONFIG_DATA_TYPE" != "" ]]; then
                CONFIG_DATA_TYPES=$CONFIG_DATA_TYPE,$CONFIG_DATA_TYPES
                CONFIG_FILES=${config_file##*/},$CONFIG_FILES
            fi
        done
    fi
    
    BULK_MAP_OUTPUT_COMPRESS=${BULK_MAP_OUTPUT_COMPRESS}
    BULK_MAP_OUTPUT_COMPRESS=${BULK_MAP_OUTPUT_COMPRESS:-true}
    BULK_MAP_OUTPUT_COMPRESSION_CODEC=${BULK_MAP_OUTPUT_COMPRESSION_CODEC}
    BULK_MAP_OUTPUT_COMPRESSION_CODEC=${BULK_MAP_OUTPUT_COMPRESSION_CODEC:-org.apache.hadoop.io.compress.DefaultCodec}
    BULK_MAP_OUTPUT_COMPRESSION_TYPE=${BULK_MAP_OUTPUT_COMPRESSION_TYPE}
    BULK_MAP_OUTPUT_COMPRESSION_TYPE=${BULK_MAP_OUTPUT_COMPRESSION_TYPE:-BLOCK}
    
    LIVE_MAP_OUTPUT_COMPRESS=${LIVE_MAP_OUTPUT_COMPRESS}
    LIVE_MAP_OUTPUT_COMPRESS=${LIVE_MAP_OUTPUT_COMPRESS:-true}
    LIVE_MAP_OUTPUT_COMPRESSION_CODEC=${LIVE_MAP_OUTPUT_COMPRESSION_CODEC}
    LIVE_MAP_OUTPUT_COMPRESSION_CODEC=${LIVE_MAP_OUTPUT_COMPRESSION_CODEC:-org.apache.hadoop.io.compress.DefaultCodec}
    LIVE_MAP_OUTPUT_COMPRESSION_TYPE=${LIVE_MAP_OUTPUT_COMPRESSION_TYPE}
    LIVE_MAP_OUTPUT_COMPRESSION_TYPE=${LIVE_MAP_OUTPUT_COMPRESSION_TYPE:-RECORD}
    
    MISSION_MGMT_DATA_TYPES="${MISSION_MGMT_DATA_TYPES}"
    
    BULK_INGEST_REDUCERS="10"
    LIVE_INGEST_REDUCERS="10"
    
    declare -i INGEST_BULK_MAPPERS=4
    declare -i INGEST_MAX_BULK_BLOCKS_PER_JOB=4
    declare -i INGEST_LIVE_MAPPERS=4
    declare -i INGEST_MAX_LIVE_BLOCKS_PER_JOB=4
    
    MAP_LOADER_HDFS_NAME_NODES="${MAP_LOADER_HDFS_NAME_NODES}"
    MAP_LOADER_HDFS_NAME_NODES="${MAP_LOADER_HDFS_NAME_NODES:-$WAREHOUSE_HDFS_NAME_NODE}"
    NUM_MAP_LOADERS="1"
    NUM_MAP_LOADERS="${NUM_MAP_LOADERS:-1}"
    
    ZOOKEEPER_HOME="/usr/lib/zookeeper"
    
    JAVA_HOME="/usr/lib/jvm/java/"
    PYTHON="/usr/bin/python"
    
    HDFS_BASE_DIR="/data"
    
    BASE_WORK_DIR="${BASE_WORK_DIR}"
    BASE_WORK_DIR="${BASE_WORK_DIR:-/datawave/ingest/work}"
    
    HDFS_MONITOR_ARGS="${HDFS_MONITOR_ARGS}"
    
    MONITOR_SERVER_HOST="monitor"
    MONITOR_ENABLED="${MONITOR_ENABLED}"
    MONITOR_ENABLED="${MONITOR_ENABLED:-true}"
    
    LOG_DIR="/srv/logs/ingest"
    PDSH_LOG_DIR="/srv/logs/ingest/pdsh_logs"
    FLAG_DIR="/srv/data/datawave/flags"
    BIN_DIR_FOR_FLAGS="/opt/datawave-ingest/current/bin"
    FLAG_MAKER_CONFIG="/opt/datawave-ingest/current/config/flag-maker-live.xml,/opt/datawave-ingest/current/config/flag-maker-bulk.xml"
    FLAG_EXTRA_ARGS=""
    
    declare -i NUM_SHARDS=${NUM_SHARDS}
    declare -i NUM_DATE_INDEX_SHARDS=${NUM_DATE_INDEX_SHARDS}
    
    MAP_LOADER_MAJC_THRESHOLD="${MAP_LOADER_MAJC_THRESHOLD}"
    MAP_LOADER_MAJC_THRESHOLD="${MAP_LOADER_MAJC_THRESHOLD:-32000}"
    
    # using ../ingest instead of ./ allows scripts in other bin directories to use this
    findVersion (){
      ls -1 $1/$2-*.jar | grep -v sources | grep -v javadoc | sort | tail -1 | sed 's/.*\///' | sed "s/$2-//" | sed 's/.jar//'
    }
    findHadoopVersion (){
      $1/bin/hadoop version | head -1 | awk '{print $2}'
    }
    METRICS_VERSION=$(findVersion ../../lib datawave-metrics-core)
    INGEST_VERSION=$(findVersion ../../lib datawave-ingest-csv)
    HADOOP_VERSION=$(findHadoopVersion $INGEST_HADOOP_HOME)
    
    
    # Turn some of the comma delimited lists into arrays
    OLD_IFS="$IFS"
    IFS=","
    FLAG_MAKER_CONFIG=( $FLAG_MAKER_CONFIG )
    CONFIG_DATA_TYPES=( $CONFIG_DATA_TYPES )
    CONFIG_FILES=( $CONFIG_FILES )
    MAP_LOADER_HDFS_NAME_NODES=( $MAP_LOADER_HDFS_NAME_NODES )
    NUM_MAP_LOADERS=( $NUM_MAP_LOADERS )
    IFS="$OLD_IFS"
    
    # Export the variables as needed (required by some python scripts and some java code)
    export LOG_DIR
    export FLAG_DIR
    export BIN_DIR_FOR_FLAGS
    export INGEST_HDFS_NAME_NODE HDFS_BASE_DIR BASE_WORK_DIR
    export INGEST_BULK_MAPPERS INGEST_MAX_BULK_BLOCKS_PER_JOB
    export INGEST_LIVE_MAPPERS INGEST_MAX_LIVE_BLOCKS_PER_JOB
    export BULK_INGEST_REDUCERS LIVE_INGEST_REDUCERS MISSION_MGMT_INGEST_REDUCERS
    export BULK_INGEST_DATA_TYPES LIVE_INGEST_DATA_TYPES MISSION_MGMT_DATA_TYPES
    export MONITOR_SERVER_HOST MONITOR_ENABLED
    export PYTHON INGEST_HADOOP_HOME WAREHOUSE_HADOOP_HOME JAVA_HOME
    export NUM_SHARDS NUM_DATE_INDEX_SHARDS
    export INDEX_STATS_MAX_MAPPERS
    
    BULK_CHILD_MAP_MAX_MEMORY_MB="2048"
    BULK_CHILD_MAP_MAX_MEMORY_MB="${BULK_CHILD_MAP_MAX_MEMORY_MB:-$BULK_CHILD_MAX_MEMORY_MB}"
    LIVE_CHILD_MAP_MAX_MEMORY_MB="1024"
    LIVE_CHILD_MAP_MAX_MEMORY_MB="${LIVE_CHILD_MAP_MAX_MEMORY_MB:-$LIVE_CHILD_MAX_MEMORY_MB}"
    
    BULK_CHILD_REDUCE_MAX_MEMORY_MB="2048"
    BULK_CHILD_REDUCE_MAX_MEMORY_MB="${BULK_CHILD_REDUCE_MAX_MEMORY_MB:-$BULK_CHILD_MAX_MEMORY_MB}"
    LIVE_CHILD_REDUCE_MAX_MEMORY_MB="1024"
    LIVE_CHILD_REDUCE_MAX_MEMORY_MB="${LIVE_CHILD_REDUCE_MAX_MEMORY_MB:-$LIVE_CHILD_MAX_MEMORY_MB}"
    
    DEFAULT_IO_SORT_MB="${DEFAULT_IO_SORT_MB}"
    DEFAULT_IO_SORT_MB="${DEFAULT_IO_SORT_MB:-"768"}"
    BULK_CHILD_IO_SORT_MB="${BULK_CHILD_IO_SORT_MB}"
    BULK_CHILD_IO_SORT_MB="${BULK_CHILD_IO_SORT_MB:-$DEFAULT_IO_SORT_MB}"
    LIVE_CHILD_IO_SORT_MB="${LIVE_CHILD_IO_SORT_MB}"
    LIVE_CHILD_IO_SORT_MB="${LIVE_CHILD_IO_SORT_MB:-$DEFAULT_IO_SORT_MB}"
    
    COMPOSITE_INGEST_DATA_TYPES=""
    DEPRECATED_INGEST_DATA_TYPES=""
    
    BULK_INGEST_GROUPING="${BULK_INGEST_GROUPING}"
    BULK_INGEST_GROUPING="${BULK_INGEST_GROUPING:-none}"
    LIVE_INGEST_GROUPING="${LIVE_INGEST_GROUPING}"
    LIVE_INGEST_GROUPING="${LIVE_INGEST_GROUPING:-none}"
    
    INGEST_BULK_JOBS="1"
    INGEST_LIVE_JOBS="1"
    
    BULK_INGEST_TIMEOUT_SECS="${BULK_INGEST_TIMEOUT_SECS}"
    BULK_INGEST_TIMEOUT_SECS="${BULK_INGEST_TIMEOUT_SECS:-300}"
    LIVE_INGEST_TIMEOUT_SECS="${LIVE_INGEST_TIMEOUT_SECS}"
    LIVE_INGEST_TIMEOUT_SECS="${LIVE_INGEST_TIMEOUT_SECS:-10}"
    
    declare -i INDEX_STATS_MAX_MAPPERS=1
    
    # Export the variables as needed (required by some python scripts and some java code)
    export INGEST_BULK_JOBS
    export INGEST_LIVE_JOBS
    export BULK_INGEST_GROUPING LIVE_INGEST_GROUPING
    export BULK_INGEST_TIMEOUT_SECS LIVE_INGEST_TIMEOUT_SECS
    
    # CERT required for various download scripts (PEM format)
    export SERVER_CERT="${SERVER_CERT}"
    export KEYSTORE="/data/certs/keystore.p12"
    export KEYSTORE_TYPE="PKCS12"
    export TRUSTSTORE="/data/certs/truststore.jks"
    export TRUSTSTORE_TYPE="JKS"
    
    
    LOAD_JOBCACHE_CPU_MULTIPLIER="${LOAD_JOBCACHE_CPU_MULTIPLIER}"
    declare -i LOAD_JOBCACHE_CPU_MULTIPLIER=${LOAD_JOBCACHE_CPU_MULTIPLIER:-2}
    
    # some functions used by script to parse flag file names
    
    isNumber() {
      re='^[0-9]+$'
      if [[ $1 =~ $re ]]; then
        echo "true"
      else
        echo "false"
      fi
    }
    
    flagPipeline() {
      BASENAME=${1%.*}
      PIPELINE=${BASENAME##*.}
      if [[ $(isNumber $PIPELINE) == "true" ]]; then
        echo $PIPELINE
      else
        echo 0
      fi
    }
    
    flagBasename() {
      f=$1
      BASENAME=${f%%.flag*}
      echo $BASENAME
    }
    
    # Source script containing lock file acquisition logic
    . ../util/file_locker.sh
    
    # Call the function to acquire the lock on a lock file with the program name
    # for CRON mutual exclusiveness.
    # This function should be called in this way: `acquire_lock_file $(basename "$0")`
    function acquire_lock_file() {
      lock $1
      return $?
    }

  files:
  - name: all-config.xml
    properties:
      - name: all.handler.classes
        value: >-
          datawave.ingest.mapreduce.handler.edge.ProtobufEdgeDataTypeHandler,datawave.ingest.mapreduce.handler.dateindex.DateIndexDataTypeHandler
        description: >-
          Datatype handlers to be utilized by all registered datatypes (in
          *addition* to any distinct handlers
                  that they may have set independently via their respective *-config.xml files)
      - name: all.ingest.helper.class
        value: ''
      - name: all.reader.class
        value: ''
      - name: all.data.category.marking.upstream.error
        value: SOMEDOMAIN=SOMEMARKINGASS
        description: >-
          If any record comes into the system with this security marking, then
          recognize it as an upstream error
                  which is considered to be FATAL in this system
      - description: Properties to include in the accumulo table config cache
        name: cache.table.properties
        value: >-
          table.file.compress.*,table.file.blocksize,table.file.replication,table.iterator.minc.*,crypto.*
      - name: all.filter.classes
        value: ${ALL_FILTER_CLASSES}
        description: >-
          This is the chain of context writers that will receive the output of
          all handlers and
                  higher prioriy content writers
      - name: all.filter.priority
        value: 50
        description: This is the priority of this context writer chain
      - name: all.data.combine.separator
        value: ':'
        description: Output separator for Virtual Fields
      - name: all.ingest.policy.enforcer.class
        value: datawave.policy.IngestPolicyEnforcer$NoOpIngestPolicyEnforcer
        description: |-
          Name of the class to use for policy enforcement.
                  (1) datawave.policy.IngestPolicyEnforcer$NoOpIngestPolicyEnforcer will assume all records are valid.
                  (2) datawave.policy.ExampleIngestPolicyEnforcer will perform some validations that you'd probably want
                      to enforce for all data in a production deployment
      - name: all.date.index.type.to.field.map
        value: LOADED=LOAD_DATE,ACTIVITY=EVENT_DATE
  - name: edge-ingest-config.xml
    properties:
    - name: protobufedge.table.name
      value: datawave.edge
    - name: protobufedge.table.loader.priority
      value: 30
    - name: datawave.edge.table.config.class
      value: datawave.ingest.table.config.ProtobufEdgeTableConfigHelper
    - name: protobufedge.table.metadata.enable
      value: true
    - name: protobufedge.table.disallowlist.enable
      value: true
    - name: protobufedge.spring.config
      value: config/edge-definitions.xml
    - name: protobufedge.setup.default.failurepolicy
      value: FAIL_JOB
    - name: protobufedge.process.default.failurepolicy
      value: FAIL_JOB
    - name: protobufedge.valid.activitytime.future.delta
      value: "86400000"
    - name: protobufedge.valid.activitytime.past.delta
      value: "315360000000"
  - name: shard-ingest-config.xml
    properties:
    - name: num.shards
      value: 10
    - name: sharded.table.names
      value: datawave.shard,datawave.error_s
      description: Comma-separated list of tables that need to pull splits from accumulo
    - name: shard.table.name
      value: datawave.shard
    - name: shard.index.create.uids
      value: true
    - name: shard.table.loader.priority
      value: 30
    - name: datawave.shard.table.config.class
      value: datawave.ingest.table.config.ShardTableConfigHelper
    - name: shard.table.locality.groups
      value: fullcontent:d,termfrequency:tf
      description: >-
        The list of locality groups in the form groupname:columnfamily, comma
        separated
    - name: shard.global.index.table.name
      value: datawave.shardIndex
    - name: shard.global.index.table.loader.priority
      value: 30
    - name: datawave.shardIndex.table.config.class
      value: datawave.ingest.table.config.ShardTableConfigHelper
    - name: shard.global.rindex.table.name
      value: datawave.shardReverseIndex
    - name: shard.global.rindex.table.loader.priority
      value: 30
    - name: datawave.shardReverseIndex.table.config.class
      value: datawave.ingest.table.config.ShardTableConfigHelper
    - name: markings.setup.iterator.enabled
      value: false
    - name: markings.setup.iterator.config
      value: ''
    - name: shard.global.index.geo.field
      value: LAT_LONG
    - name: partitioner.category.shardedTables
      value: datawave.ingest.mapreduce.partition.BalancedShardPartitioner
    - name: partitioner.category.member.datawave.shard
      value: shardedTables
    - name: index.tables.keep.count.only.entries
      value: false
  - name: metadata-config.xml
    properties:
    - name: metadata.ingest.helper.class
      value: datawave.ingest.data.config.ingest.MetaDataIngestHelper
    - name: metadata.reader.class
      value: ''
    - name: metadata.handler.classes
      value: ''
    - name: metadata.table.name
      value: datawave.metadata
    - name: metadata.table.loader.priority
      value: 30
    - name: datawave.metadata.table.config.class
      value: datawave.ingest.table.config.MetadataTableConfigHelper
    - name: metadata.loaddates.enabled
      value: true
    - name: metadata.loaddates.table.name
      value: datawave.loadDates
    - name: metadata.loaddates.table.loader.priority
      value: 30
    - name: metadata.loaddates.table.locality.groups
      value: ''
      description: >-
        The list of locality groups.  Groups are comma separated.  Column
        families are semicolon separated, e.g.
        groupName1:colFamily1;colFamily2,groupName2:colFamily3
    - name: datawave.loadDates.table.config.class
      value: datawave.ingest.table.config.LoadDateTableConfigHelper
    - name: metadata.term.frequency.enabled
      value: true
  - name: facet-config.xml
    properties:
    - name: facet.table.name
      value: datawave.facets
    - name: facet.table.loader.priority
      value: 40
    - name: facet.metadata.table.name
      value: datawave.facetMetadata
    - name: facet.metadata.table.loader.priority
      value: 40
    - name: facet.hash.table.name
      value: datawave.facetHashes
    - name: facet.hash.table.loader.priority
      value: 40
  - name: error-ingest-config.xml
    properties:
    - name: error.table.loader.priority
      value: 30
    - name: error.shard.table.name
      value: datawave.error_s
    - name: error.shard.table.loader.priority
      value: 30
    - name: error.shard.global.index.table.name
      value: datawave.error_i
    - name: error.shard.global.index.table.loader.priority
      value: 30
    - name: error.shard.global.rindex.table.name
      value: datawave.error_r
    - name: error.shard.global.rindex.table.loader.priority
      value: 30
    - name: error.shard.enable.bloom.filters
      value: false
      description: >-
        Whether or not to add bloom filters on the index tables. Default is
        false
    - name: error.shard.table.locality.groups
      value: fullcontent:d
      description: >-
        The list of locality groups in the form groupname:columnfamily, comma
        separated
    - name: error.metadata.table.name
      value: datawave.error_m
    - name: error.metadata.table.loader.priority
      value: 30
    - name: error.metadata.term.frequency.enabled
      value: true
    - name: datawave.error_s.table.config.class
      value: datawave.ingest.table.config.ErrorShardTableConfigHelper
    - name: datawave.error_i.table.config.class
      value: datawave.ingest.table.config.ErrorShardTableConfigHelper
    - name: datawave.error_r.table.config.class
      value: datawave.ingest.table.config.ErrorShardTableConfigHelper
    - name: datawave.error_m.table.config.class
      value: datawave.ingest.table.config.ErrorMetadataTableConfigHelper
    - name: error.table.ageoff.ttl
      value: 30
    - name: error.data.default.type.class
      value: datawave.data.type.LcNoDiacriticsType
    - name: error.table.ageoff.ttlunits
      value: d
    - name: error.use.default.type.handlers
      value: true
    - name: error.ingest.helper.class
      value: datawave.ingest.data.config.ingest.ErrorShardedIngestHelper
    - name: error.reader.class
      value: ''
    - name: error.handler.classes
      value: >-
        datawave.ingest.mapreduce.handler.error.ErrorShardedDataTypeHandler,${ADDITIONAL_ERROR_HANDLER_CLASSES}
    - name: error.data.category.marking.default
      value: PRIVATE
      description: >-
        Default ColumnVisibility to be applied to fields/records if none
        provided in the data
    - name: error.data.replace.malformed.utf8
      value: true
    - name: partitioner.category.member.datawave.error_s
      value: shardedTables
  - name : dateindex-ingest-config.xml
    properties:
      - name: date.index.table.name
        value: datawave.dateIndex
      - name: date.index.table.loader.priority
        value: 30
      - name: datawave.dateIndex.table.config.class
        value: datawave.ingest.table.config.DateIndexTableConfigHelper
      - name: date.index.table.locality.groups
        value: activity:ACTIVITY,loaded:LOADED
        description: >-
          The list of locality groups in the form groupname:columnfamily, comma separated
      - name: partitioner.dedicated.datawave.dateIndex
        value: datawave.ingest.mapreduce.partition.LimitedKeyPartitioner
      - name: date.index.num.shards
        value: 10
      - name: datawave.dateIndex.partition.limiter.max
        value: 3

  overallFlagConfig:
    defaultCfg:
      live:
        maxFlags: 4
        reducers: 10
        script: bin/ingest/live-ingest.sh
        fileListMarker: "***FILE_LIST***"
        collectMetrics: false
      bulk:
        maxFlags: 4
        reducers: 10
        script: bin/ingest/bulk-ingest.sh
        fileListMarker: "***FILE_LIST***"
        collectMetrics: false
    properties:
      live:
        timeoutMilliSecs: 10000
        baseHDFSDir: /data
        distributorType: simple
        filePattern: "[0-9a-zA-Z]*[0-9a-zA-Z]"
        hdfs: hdfs://hdfs-nn:9000
        socketPort: 20000
        datawaveHome: /opt/datawave-ingest/current
        flagFileDirectory: /srv/data/datawave/flags
        setFlagFileTimestamp: false
        useFolderTimestamp: false
      bulk:
        sleepMilliSecs: 5000
        timeoutMilliSecs: 480000
        baseHDFSDir: /data
        distributorType: simple
        filePattern: "[0-9a-zA-Z]*[0-9a-zA-Z]"
        hdfs: hdfs://hdfs-nn:9000
        socketPort: 20001
        datawaveHome: /opt/datawave-ingest/current
        flagFileDirectory: /srv/data/datawave/flags
        setFlagFileTimestamp: false
        useFolderTimestamp: false
        
  types:
    - name: openlib
      flagMakerConfig:
        dataName: openlib
        liveFolder: openlib
        bulkFolder: openlib-bulk
        config:
          distrubutionArgs: none
          extraIngestArgs: "-data.name.override=openlib"
          inputFormat: datawave.ingest.json.mr.input.JsonInputFormat
          lifo: false
      properties: 
        "file.input.format": datawave.ingest.json.mr.input.JsonInputFormat
        "data.name": openlib
        "openlib.output.name": openlib
        "openlib.ingest.helper.class": datawave.ingest.json.config.helper.JsonIngestHelper
        "openlib.reader.class": datawave.ingest.json.mr.input.JsonRecordReader
        "openlib.handler.classes": "datawave.ingest.json.mr.handler.ContentJsonColumnBasedHandler,datawave.ingest.mapreduce.handler.facet.FacetHandler"
        "openlib.data.category.uuid.fields": KEY
        "openlib.data.separator": ","
        "openlib.data.header": KEY,TYPE,LASTMODIFIED_VALUE.LASTMODIFIED_0.VALUE_0
        "openlib.data.process.extra.fields": true
        "openlib.data.json.flattener.mode": GROUPED_AND_NORMAL
        "openlib.data.category.marking.default": PRIVATE|(BAR&amp;FOO)
        "openlib.data.category.date": LASTMODIFIED_VALUE.LASTMODIFIED_0.VALUE_0
        "openlib.data.category.date.formats": yyyy-MM-dd,yyyy-MM-dd'T'HH:mm:ss'Z',yyyy-MM-dd HH:mm:ss
        "openlib.data.category.index": KEY,NAME,TITLE,SUBTITLE,TYPE,ISBN_10
        "openlib.data.category.index.reverse": KEY,NAME,TITLE,SUBTITLE,TYPE,ISBN_10
        "openlib.data.category.token.fieldname.designator": _TOKEN
        "openlib.data.category.index.tokenize.allowlist": KEY,TITLE,SUBTITLE
        "openlib.data.category.index.only": KEY_TOKEN,TITLE_TOKEN,SUBTITLE_TOKEN
        "openlib.data.default.normalization.failure.policy": FAIL
        "openlib.data.default.type.class": datawave.data.type.LcNoDiacriticsType
        "openlib.LASTMODIFIED_VALUE.LASTMODIFIED_0.VALUE_0.data.field.type.class": datawave.data.type.DateType
        "openlib.facet.category.name.network": ""  
    - name: myjson
      flagMakerConfig:
        dataName: myjson
        liveFolder: myjson
        bulkFolder: myjson-bulk
        config:
          distrubutionArgs: none
          extraIngestArgs: "-data.name.override=myjson"
          inputFormat: datawave.ingest.json.mr.input.JsonInputFormat
          lifo: false
      properties: 
        "file.input.format": datawave.ingest.json.mr.input.JsonInputFormat
        "data.name": myjson
        "myjson.output.name": tvmaze
        "myjson.ingest.helper.class": datawave.ingest.json.config.helper.JsonIngestHelper
        "myjson.reader.class": datawave.ingest.json.mr.input.JsonRecordReader
        "myjson.handler.classes": "datawave.ingest.json.mr.handler.ContentJsonColumnBasedHandler,datawave.ingest.mapreduce.handler.facet.FacetHandler"
        "myjson.data.category.uuid.fields": ID,EXTERNALS_THETVDB,EXTERNALS_TVRAGE,EXTERNALS_IMDB
        "myjson.data.separator": ","
        "myjson.data.header": ID,NAME,PREMIERED,RUNTIME,STATUS,SUMMARY,OFFICIALSITE,LANGUAGE,GENRES,WEIGHT,URL,TYPE
        "myjson.data.process.extra.fields": true
        "myjson.data.json.flattener.mode": GROUPED_AND_NORMAL
        "myjson.data.category.marking.default": PRIVATE|(BAR&amp;FOO)
        "myjson.SUMMARY.data.field.marking": PUBLIC
        "myjson.data.category.date": PREMIERED
        "myjson.data.category.date.formats": yyyy-MM-dd,yyyy-MM-dd'T'HH:mm:ss'Z',yyyy-MM-dd HH:mm:ss
        "myjson.data.category.index": NAME,ID,ID,EXTERNALS_THETVDB,EXTERNALS_TVRAGE,EXTERNALS_IMDB,EMBEDDED_CAST_CHARACTER_NAME,EMBEDDED_CAST_PERSON_NAME,EMBEDDED_CAST_PERSON_ID,GENRES,NETWORK_NAME,OFFICIALSITE,TYPE,STATUS,RUNTIME,URL
        "myjson.data.category.index.reverse": NAME,NETWORK_NAME,OFFICIALSITE,URL
        "myjson.data.category.token.fieldname.designator": _TOKEN
        "myjson.data.category.index.tokenize.allowlist": SUMMARY,NETWORK_NAME,NAME,EMBEDDED_CAST_CHARACTER_NAME,EMBEDDED_CAST_PERSON_NAME
        "myjson.data.category.index.only": SUMMARY_TOKEN,NETWORK_NAME_TOKEN,NAME_TOKEN,EMBEDDED_CAST_CHARACTER_NAME_TOKEN,EMBEDDED_CAST_PERSON_NAME_TOKEN
        "myjson.data.default.normalization.failure.policy": FAIL
        "myjson.data.default.type.class": datawave.data.type.LcNoDiacriticsType
        "myjson.PREMIERED.data.field.type.class": datawave.data.type.DateType
        "myjson.WEIGHT.data.field.type.class": datawave.data.type.NumberType
        "myjson.RUNTIME.data.field.type.class": datawave.data.type.NumberType
        "myjson.facet.category.name.network": "NETWORK_NAME;GENRES,EMBEDDED_CAST_PERSON_GENDER,RATING_AVERAGE"  
    - name: mycsv
      flagMakerConfig:
        dataName: mycsv
        liveFolder: mycsv
        bulkFolder: mycsv-bulk
        config:
          distrubutionArgs: none
          extraIngestArgs: "-data.name.override=mycsv"
          inputFormat: datawave.ingest.csv.mr.input.CSVFileInputFormat
          lifo: false
      properties: 
        "file.input.format": datawave.ingest.csv.mr.input.CSVFileInputFormat
        "data.name": mycsv
        "mycsv.output.name": csv
        "mycsv.ingest.helper.class": datawave.ingest.json.config.helper.JsonIngestHelper
        "mycsv.reader.class": datawave.ingest.json.mr.input.JsonRecordReader
        "mycsv.handler.classes": "datawave.ingest.json.mr.handler.ContentJsonColumnBasedHandler,datawave.ingest.mapreduce.handler.facet.FacetHandler"
        "mycsv.data.category.uuid.fields": ID,EXTERNALS_THETVDB,EXTERNALS_TVRAGE,EXTERNALS_IMDB
        "mycsv.data.separator": ","
        "mycsv.data.header": ID,NAME,PREMIERED,RUNTIME,STATUS,SUMMARY,OFFICIALSITE,LANGUAGE,GENRES,WEIGHT,URL,TYPE
        "mycsv.data.process.extra.fields": true
        "mycsv.data.json.flattener.mode": GROUPED_AND_NORMAL"
        "mycsv.data.category.marking.default": PRIVATE|(BAR&amp;FOO)
        "mycsv.SUMMARY.data.field.marking": PUBLIC
        "mycsv.data.category.date.formats": yyyy-MM-dd,yyyy-MM-dd'T'HH:mm:ss'Z',yyyy-MM-dd HH:mm:ss
        "mycsv.data.category.index": NAME,ID,ID,EXTERNALS_THETVDB,EXTERNALS_TVRAGE,EXTERNALS_IMDB,EMBEDDED_CAST_CHARACTER_NAME,EMBEDDED_CAST_PERSON_NAME,EMBEDDED_CAST_PERSON_ID,GENRES,NETWORK_NAME,OFFICIALSITE,TYPE,STATUS,RUNTIME,URL
        "mycsv.data.category.index.reverse": NAME,NETWORK_NAME,OFFICIALSITE,URL
        "mycsv.data.category.token.fieldname.designator": _TOKEN
        "mycsv.data.category.index.tokenize.allowlist": SUMMARY,NETWORK_NAME,NAME,EMBEDDED_CAST_CHARACTER_NAME,EMBEDDED_CAST_PERSON_NAME
        "mycsv.data.category.index.only": SUMMARY_TOKEN,NETWORK_NAME_TOKEN,NAME_TOKEN,EMBEDDED_CAST_CHARACTER_NAME_TOKEN,EMBEDDED_CAST_PERSON_NAME_TOKEN
        "mycsv.data.default.normalization.failure.policy": FAIL
        "mycsv.data.default.type.class": datawave.data.type.LcNoDiacriticsType
        "mycsv.PREMIERED.data.field.type.class": datawave.data.type.DateType
        "mycsv.WEIGHT.data.field.type.class": datawave.data.type.NumberType
        "mycsv.RUNTIME.data.field.type.class": datawave.data.type.NumberType
        "mycsv.facet.category.name.network": "NETWORK_NAME;GENRES,EMBEDDED_CAST_PERSON_GENDER,RATING_AVERAGE"    