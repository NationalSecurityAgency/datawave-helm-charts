apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "hadoop.fullname" . }}
  labels:
    app: {{ include "hadoop.name" . }}
    chart: {{ include "hadoop.chart" . }}
    release: {{ .Release.Name }}
data:
  bootstrap.sh: |
    #!/bin/bash

    : ${HADOOP_HOME:=/usr/local/hadoop}

    : ${HADOOP_PREFIX:=/usr/local/hadoop}

    . $HADOOP_HOME/etc/hadoop/hadoop-env.sh

    # Directory to find config artifacts
    CONFIG_DIR="/tmp/hadoop-config"

    # Copy config files from volume mount

    for f in slaves core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml; do
      if [[ -e ${CONFIG_DIR}/$f ]]; then
        cp ${CONFIG_DIR}/$f $HADOOP_HOME/etc/hadoop/$f
      else
        echo "ERROR: Could not find $f in $CONFIG_DIR"
        exit 1
      fi
    done

    # installing libraries if any - (resource urls added comma separated to the ACP system variable)
    cd $HADOOP_HOME/ ; for cp in ${ACP//,/ }; do  echo == $cp; curl -LO $cp ; done; cd -

    if [[ "${HOSTNAME}" =~ "{{ .Values.hadoop.hdfs.nameNode.hostname }}" ]]; then
      echo "Check if Namenode directory exists"
      if [[ ! -d /usr/local/hadoop/data ]]; then
        echo "Creating Namenode directory"
        mkdir -p /usr/local/hadoop/data/
      else
        echo "/usr/local/hadoop/data/ exists"
      fi
      if [[ ! -f /usr/local/hadoop/data/current/VERSION ]]; then
        echo "Formatting..."
        $HADOOP_HDFS_HOME/bin/hdfs namenode -format -force -nonInteractive
      else
        echo "/usr/local/hadoop/data/ exists"
        echo $(ls /usr/local/hadoop/data/)
        echo $(ls /usr/local/hadoop/data/current)
      fi
      echo "END IFS"
      echo "Start NameNode"
      $HADOOP_PREFIX/sbin/hadoop-daemon.sh start namenode
    fi

    if [[ "${HOSTNAME}" =~ "hdfs-dn" ]]; then
      echo "Check if Datanode directory exists"
      if [[ ! -d /usr/local/hadoop/hdfs/datanode ]]; then
        echo "Creating Datanode directory"
        mkdir -p /usr/local/hadoop/hdfs/datanode
      fi
      #  wait up to 30 seconds for namenode
      (while [[ $count -lt 15 && -z `curl -sf http://{{ .Values.hadoop.hdfs.nameNode.serviceName }}:9870/dfshealth.html` ]]; do ((count=count+1)) ; echo "Waiting for {{ .Values.hadoop.hdfs.nameNode.serviceName }}" ; sleep 2; done && [[ $count -lt 15 ]])
      [[ $? -ne 0 ]] && echo "Timeout waiting for {{ .Values.hadoop.hdfs.nameNode.serviceName }}, exiting." && exit 1

      $HADOOP_HOME/sbin/hadoop-daemon.sh start datanode
      $HADOOP_HDFS_HOME/bin/hdfs dfs -chmod 777 /
    fi

    if [[ "${HOSTNAME}" =~ "yarn-rm" ]]; then
      (while [[ $count -lt 15 && -z `curl -sf http://{{ .Values.hadoop.hdfs.nameNode.serviceName }}:9870/dfshealth.html` ]]; do ((count=count+1)) ; echo "Waiting for {{ .Values.hadoop.hdfs.nameNode.serviceName }}" ; sleep 2; done && [[ $count -lt 15 ]])
      [[ $? -ne 0 ]] && echo "Timeout waiting for {{ .Values.hadoop.hdfs.nameNode.serviceName }}, exiting." && exit 1
      cp ${CONFIG_DIR}/start-yarn-rm.sh $HADOOP_YARN_HOME/sbin/
      cd $HADOOP_YARN_HOME/sbin
      chmod +x start-yarn-rm.sh
      ./start-yarn-rm.sh
    fi

    if [[ "${HOSTNAME}" =~ "yarn-nm" ]]; then
      sed -i '/<\/configuration>/d' $HADOOP_HOME/etc/hadoop/yarn-site.xml
      cat >> $HADOOP_HOME/etc/hadoop/yarn-site.xml <<- EOM
      <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>${MY_MEM_LIMIT:-4096}</value>
      </property>

      <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>${MY_CPU_LIMIT:-2}</value>
      </property>
    EOM
      echo '</configuration>' >> $HADOOP_HOME/etc/hadoop/yarn-site.xml
      cp ${CONFIG_DIR}/start-yarn-nm.sh $HADOOP_YARN_HOME/sbin/
      cd $HADOOP_YARN_HOME/sbin
      chmod +x start-yarn-nm.sh

      #  wait up to 30 seconds for resourcemanager
      (while [[ $count -lt 15 && -z `curl -sf http://yarn-rm:8088/ws/v1/cluster/info` ]]; do ((count=count+1)) ; echo "Waiting for yarn-rm" ; sleep 2; done && [[ $count -lt 15 ]])
      [[ $? -ne 0 ]] && echo "Timeout waiting for yarn-rm, exiting." && exit 1

      ./start-yarn-nm.sh
    fi

    if [[ $1 == "-d" ]]; then
      until find ${HADOOP_HOME}/logs -mmin -1 | egrep -q '.*'; echo "`date`: Waiting for logs..." ; do sleep 2 ; done
      tail -F ${HADOOP_HOME}/logs/* &
      while true; do sleep 1000; done
    fi

    if [[ $1 == "-bash" ]]; then
      /bin/bash
    fi

  log4j.properties: |
    # Define some default values that can be overridden by system properties
    hadoop.root.logger=INFO,console
    hadoop.log.dir=.
    hadoop.log.file=hadoop.log
    
    # Define the root logger to the system property "hadoop.root.logger".
    log4j.rootLogger=${hadoop.root.logger}
    
    # Logging Threshold
    log4j.threshhold=ALL
    
    #
    # Daily Rolling File Appender
    #
    
    log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender
    log4j.appender.DRFA.File=${hadoop.log.dir}/${hadoop.log.file}
    log4j.appender.DRFA.DatePattern=.yyyy-MM-dd
    
    # 30-day backup
    #log4j.appender.DRFA.MaxBackupIndex=30
    log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout
    
    # Pattern format: Date LogLevel LoggerName LogMessage
    log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
    # Debugging Pattern format
    #log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n
    
    #
    # console
    # Add "console" to rootlogger above if you want to use this 
    #
    
    log4j.appender.console=org.apache.log4j.ConsoleAppender
    log4j.appender.console.target=System.err
    log4j.appender.console.layout=org.apache.log4j.PatternLayout
    log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n
    
    #
    # TaskLog Appender
    #
    
    #Default values
    hadoop.tasklog.taskid=null
    hadoop.tasklog.noKeepSplits=4
    hadoop.tasklog.totalLogFileSize=100
    hadoop.tasklog.purgeLogSplits=true
    hadoop.tasklog.logsRetainHours=12
    hadoop.tasklog.iscleanup=false
    
    log4j.appender.TLA=org.apache.hadoop.mapred.TaskLogAppender
    log4j.appender.TLA.taskId=${hadoop.tasklog.taskid}
    log4j.appender.TLA.totalLogFileSize=${hadoop.tasklog.totalLogFileSize}
    
    log4j.appender.TLA.isCleanup=${hadoop.tasklog.iscleanup}
    
    log4j.appender.TLA.layout=org.apache.log4j.PatternLayout
    log4j.appender.TLA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n

    
    #=======
    # security audit logging
    
    security.audit.logger=INFO,console
    log4j.category.SecurityLogger=${security.audit.logger}
    log4j.additivity.SecurityLogger=false
    log4j.appender.DRFAS=org.apache.log4j.DailyRollingFileAppender 
    log4j.appender.DRFAS.File=/var/local/hadoop/logs/${hadoop.id.str}/${hadoop.id.str}-auth.log
    log4j.appender.D RFAS.layout=org.apache.log4j.PatternLayout
    log4j.appender.DRFAS.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
    log4j.appender.DRFAS.DatePattern=.yyyy-MM-dd
    
    # hdfs audit logging 
    
    hdfs.audit.logger=INFO,console
    log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=${hdfs.audit.logger}
    log4j.additivity.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=false
    log4j.appender.DRFAAUDIT=org.apache.log4j.DailyRollingFileAppender
    log4j.appender.DRFAAUDIT.File=/var/local/hadoop/logs/hadoop-logs/hdfs-audit.log
    log4j.appender.DRFAAUDIT.layout=org.apache.log4j.PatternLayout
    log4j.appender.DRFAAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
    log4j.appender.DRFAAUDIT.DatePattern=.yyyy-MM-dd
    
    
    # mapred audit logging
    
    mapred.audit.logger=INFO,console
    log4j.logger.org.apache.hadoop.mapred.AuditLogger=${mapred.audit.logger}
    log4j.additivity.org.apache.hadoop.mapred.AuditLogger=false
    log4j.appender.MRAUDIT=org.apache.log4j.DailyRollingFileAppender
    log4j.appender.MRAUDIT.File=/var/local/hadoop/logs/hadoop-logs/mapred-audit.log
    log4j.appender.MRAUDIT.layout=org.apache.log4j.PatternLayout
    log4j.appender.MRAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
    log4j.appender.MRAUDIT.DatePattern=.yyyy-MM-dd
    
    
    # Mapred job summary 
    
    mapred.jobsummary.logger=INFO,console
    log4j.logger.org.apache.hadoop.mapred.JobInProgress$JobSummary=${mapred.jobsummary.logger}
    log4j.additivity.org.apache.hadoop.mapred.JobInProgress$JobSummary=false
    log4j.appender.JSA=org.apache.log4j.DailyRollingFileAppender
    log4j.appender.JSA.File=${hadoop.log.dir}/mapred-jobsummary.log
    log4j.appender.JSA.layout=org.apache.log4j.PatternLayout
    log4j.appender.JSA.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
    log4j.appender.JSA.DatePattern=.yyyy-MM-dd

  core-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
      <property>
            <name>fs.defaultFS</name>
            <value>hdfs://{{ .Values.hadoop.hdfs.nameNode.serviceName }}:9000/</value>
            <description>NameNode URI</description>
      </property>
      <property>
          <name>dfs.permissions.supergroup</name>
          <value>hadoop</value>
      </property>
    </configuration>

  hdfs-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
    
{{- if .Values.hadoop.hdfs.webhdfs.enabled -}}
      <property>
          <name>dfs.webhdfs.enabled</name>
          <value>true</value>
      </property> 
{{- end -}}

      <property>
        <name>dfs.datanode.use.datanode.hostname</name>
        <value>false</value>
      </property>

      <property>
        <name>dfs.client.use.datanode.hostname</name>
        <value>false</value>
      </property>

      <property>
        <name>dfs.replication</name>
          <value>3</value>
      </property>

      <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///usr/local/hadoop/hdfs/datanode</value>
        <description>DataNode directory</description>
      </property>

      <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///usr/local/hadoop/data</value>
        <description>NameNode directory for namespace and transaction logs storage.</description>
      </property>

      <property>
        <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
        <value>false</value>
      </property>

      <!-- Bind to all interfaces -->
      <property>
        <name>dfs.namenode.rpc-bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>dfs.namenode.servicerpc-bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <!-- /Bind to all interfaces -->

    </configuration>

  mapred-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

    <configuration>
      <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
      </property>
      <property>
        <name>mapreduce.map.memory.mb</name>
        <value>4096</value>
      </property>
      <property>
        <name>mapreduce.jobhistory.address</name>
        <value>yarn-rm:10020</value>
      </property>
      <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>0.0.0.0:19888</value>
      </property>
    </configuration>

  slaves: |
    localhost

  start-yarn-nm.sh: |
    #!/usr/bin/env bash

    # Licensed to the Apache Software Foundation (ASF) under one or more
    # contributor license agreements.  See the NOTICE file distributed with
    # this work for additional information regarding copyright ownership.
    # The ASF licenses this file to You under the Apache License, Version 2.0
    # (the "License"); you may not use this file except in compliance with
    # the License.  You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.


    # Start all yarn daemons.  Run this on master node.

    echo "starting yarn daemons"

    bin=`dirname "${BASH_SOURCE-$0}"`
    bin=`cd "$bin"; pwd`

    DEFAULT_LIBEXEC_DIR="$bin"/../libexec
    HADOOP_LIBEXEC_DIR=${HADOOP_LIBEXEC_DIR:-$DEFAULT_LIBEXEC_DIR}
    . $HADOOP_LIBEXEC_DIR/yarn-config.sh

    # start resourceManager
    # "$bin"/yarn-daemon.sh --config $YARN_CONF_DIR  start resourcemanager
    # start nodeManager
    "$bin"/yarn-daemon.sh --config $YARN_CONF_DIR  start nodemanager
    # start proxyserver
    #"$bin"/yarn-daemon.sh --config $YARN_CONF_DIR  start proxyserver

  start-yarn-rm.sh: |
    #!/usr/bin/env bash

    # Licensed to the Apache Software Foundation (ASF) under one or more
    # contributor license agreements.  See the NOTICE file distributed with
    # this work for additional information regarding copyright ownership.
    # The ASF licenses this file to You under the Apache License, Version 2.0
    # (the "License"); you may not use this file except in compliance with
    # the License.  You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.


    # Start all yarn daemons.  Run this on master node.

    echo "starting yarn daemons"

    bin=`dirname "${BASH_SOURCE-$0}"`
    bin=`cd "$bin"; pwd`

    DEFAULT_LIBEXEC_DIR="$bin"/../libexec
    HADOOP_LIBEXEC_DIR=${HADOOP_LIBEXEC_DIR:-$DEFAULT_LIBEXEC_DIR}
    . $HADOOP_LIBEXEC_DIR/yarn-config.sh

    # start resourceManager
    "$bin"/yarn-daemon.sh --config $YARN_CONF_DIR  start resourcemanager
    # start nodeManager
    # "$bin"/yarn-daemons.sh --config $YARN_CONF_DIR  start nodemanager
    # start proxyserver
    "$bin"/yarn-daemon.sh --config $YARN_CONF_DIR  start proxyserver

    sleep 60
    # start history server
    "$bin"/../bin/mapred --config $HADOOP_CONF_DIR  --daemon start historyserver

  yarn-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

    <configuration>
      <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>yarn-rm</value>
      </property>

      <!-- Bind to all interfaces -->
      <property>
        <name>yarn.resourcemanager.bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>yarn.nodemanager.bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>yarn.timeline-service.bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <!-- /Bind to all interfaces -->

      <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
      </property>

      <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
      </property>

      <property>
        <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
      </property>

      <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>4096</value>
      </property>
      <property>
        <description>List of directories to store localized files in.</description>
        <name>yarn.nodemanager.local-dirs</name>
        <value>/opt/hdfs/hadoop-yarn/cache/${user.name}/nm-local-dir</value>
      </property>

      <property>
        <description>Where to store container logs.</description>
        <name>yarn.nodemanager.log-dirs</name>
        <value>/opt/hdfs/hadoop-yarn/containers</value>
      </property>

      <property>
        <description>Where to aggregate logs to.</description>
        <name>yarn.nodemanager.remote-app-log-dir</name>
        <value>/opt/hdfs/hadoop-yarn/apps</value>
      </property>

      <property>
        <name>yarn.resourcemanager.scheduler.class</name>
        <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>
      </property>

      <property>
        <name>yarn.application.classpath</name>
        <value>
          /opt/hadoop/conf,
          /usr/lib/hadoop/*,
          /usr/lib/hadoop/lib/*,
          /usr/lib/hadoop-hdfs/*,
          /usr/lib/hadoop-hdfs/lib/*,
          /usr/lib/hadoop-yarn/*,
          /usr/lib/hadoop-yarn/lib/*,
          /usr/lib/hadoop-yarn/timelineservice/*,
          /usr/lib/hadoop-mapreduce/*,
          /usr/lib/hadoop-mapreduce/lib/*
        </value>
      </property>
    </configuration>

  fair-scheduler.xml: |
      <?xml version="1.0"?>
      <allocations>
        <queue name="liveIngestQueue">
          <minResources>1 mb, 1 vcores</minResources>
          <maxResources>90000 mb,0vcores</maxResources>
          <maxRunningApps>50</maxRunningApps>
          <maxAMShare>0.5</maxAMShare>
          <weight>2.0</weight>
          <schedulingPolicy>fair</schedulingPolicy
        </queue>
  
        <queueMaxAMShareDefault>0.5</queueMaxAMShareDefault>
        <queueMaxResourcesDefault>40000 mb,0vcores</queueMaxResourcesDefault>
        
        <queue name="bulkIngestQueue" type="parent">
        <weight>3.0</weight>
        <maxChildResources>4096 mb,4vcores</maxChildResources>
        </queue>
  
        <queuePlacementPolicy>
          <rule name="default" queue="liveIngestQueue"/>
        </queuePlacementPolicy>
      </allocations>
